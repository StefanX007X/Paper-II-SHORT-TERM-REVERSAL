{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b1dc5f-392b-4904-8ebb-339d7ad5d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta, datetime, time, date\n",
    "from matplotlib import rc\n",
    "from tqdm.notebook import tqdm\n",
    "from cycler import cycler\n",
    "\n",
    "tqdm().pandas()\n",
    "\n",
    "fontsize = 12\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern'], 'size': fontsize})\n",
    "rc('text', usetex=True)\n",
    "\n",
    "color_list = [\"#348ABD\",\"#A60628\",\"#7A68A6\",\"#467821\",\"#CF4457\",\"#188487\",\"#E24A33\"]\n",
    "style = {\n",
    "  \"lines.linewidth\": 2.0,\n",
    "  \"axes.edgecolor\": \"#bcbcbc\",\n",
    "  \"patch.linewidth\": 0.5,\n",
    "  \"legend.fancybox\": True,\n",
    "  \"axes.prop_cycle\": cycler('color', color_list),\n",
    "  \"axes.facecolor\": \"#ffffff\",\n",
    "  \"axes.labelsize\": \"large\",\n",
    "  \"axes.grid\": True,\n",
    "  \"patch.edgecolor\": \"#eeeeee\",\n",
    "  \"axes.titlesize\": \"x-large\",\n",
    "  \"svg.fonttype\": \"path\"}\n",
    "\n",
    "matplotlib.rcParams.update(style)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "\n",
    "data_path           = 'C:/Users/Stefa/Documents/Uni/Projektassistenz/Python/Data/'\n",
    "output_path         = 'C:/Users/Stefa/Documents/Uni/Projektassistenz/Auswertung/'\n",
    "fin_data_path       = 'C:/Users/Stefa/Documents/Uni/Projektassistenz/Financial Data/'   \n",
    "extended_model_path = data_path+'Classification/DagoBERT/SCE_Loss_minw_25_lr_5e5_3ep_bs32_wd_1e2_a0_5_b3_NN1_w2v_topics/' \n",
    "\n",
    "\n",
    "def load_data(file):\n",
    "    df = pd.read_csv(file, encoding='utf-8-sig')\n",
    "    df.Date = pd.to_datetime(df.Date)\n",
    "    df = df.set_index(['Ticker', 'Date'])\n",
    "    df = df.rename(columns={'Prediction value': 'prediction_value'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_zscores(x, idiosync_z_scores, rw=int(255/2)):\n",
    "    idiosync = '_idiosync' if idiosync_z_scores else ''\n",
    "    \n",
    "    r = x.rolling(window=rw, closed='left', min_periods=int(0.9*rw))\n",
    "    m = r.mean()\n",
    "    s = r.std()\n",
    "    z = (x-m)/s\n",
    "    z.index.name = 'Date'    \n",
    "    return z\n",
    "\n",
    "\n",
    "def correctTickers(x):\n",
    "    \"\"\"\n",
    "    Rename some tickers due to recent mergers and name changes:\n",
    "    ADS to BFH\n",
    "    DISCA to WBD\n",
    "    HFC to DINO\n",
    "    LB to BBWI\n",
    "    VIAC to PARA\n",
    "    COG to CTRA\n",
    "    \"\"\"\n",
    "    correctTickers = set(['ADS', 'DISCA', 'HFC', 'LB', 'VIAC', 'BRK.B', 'BF.B', 'BLL', 'COG'])\n",
    "    ticker_dict = {'ADS': 'BFH', 'DISCA':'WBD', 'HFC':'DINO', 'LB':'BBWI', 'VIAC':'PARA', 'BRK.B':'BRK', 'BF.B':'BF', 'BLL':'BALL', 'COG':'CTRA'}\n",
    "    if x in correctTickers:\n",
    "        return ticker_dict[x]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021376f-6974-4ab2-ac91-d2fce810a2ef",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f407c1de-a6ea-4ef7-b41b-827e63f07811",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dupl = True                # Load news events without duplicates\n",
    "return_period = 'mo_to_mc'    # mo_to_mo, mc_to_mc \n",
    "                              # mo_to_mc, mc_to_mo\n",
    "\n",
    "consider_freshness = True\n",
    "freshness = '_freshness' if consider_freshness else ''\n",
    "\n",
    "z_cols = ['z_value_t_III', 'z_value_t_II', 'z_value_t_I', 'z_value_t', 'z_value_t_1']\n",
    "r_cols = ['return_t_V', 'return_t_IV', 'return_t_III', 'return_t_II', 'return_t_I', 'return_t', 'return_t_1', \n",
    "          'return_t_2', 'return_t_3', 'return_t_4', 'return_t_5', 'return_t_6', 'return_t_7', 'return_t_8']\n",
    "n_cols = []\n",
    "\n",
    "\n",
    "# Load News Events Data\n",
    "if no_dupl:\n",
    "    news_event_df = pd.read_csv(f'./Data/news_event_df_no_dupl{freshness}.csv', encoding='utf-8', index_col=0)\n",
    "    news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "    news_event_df.Ticker = news_event_df.Ticker.apply(correctTickers)\n",
    "    news_event_df = news_event_df.set_index(['Date', 'Ticker']) \n",
    "    news_event_df = news_event_df.drop(['prev_close_date'], axis=1)    \n",
    "else:\n",
    "    news_event_df = pd.read_csv('./Data/news_event_df.csv', encoding='utf-8')\n",
    "    news_event_df.Timestamp_ET = pd.DatetimeIndex(news_event_df.Timestamp_ET)\n",
    "    news_event_df.Ticker = news_event_df.Ticker.apply(correctTickers)\n",
    "    news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "    news_event_df = news_event_df.set_index(['Timestamp_ET', 'Ticker'])\n",
    "\n",
    "\n",
    "# Return data of all constituents (file reference: Stock Return Calculation.ipynb)     \n",
    "asset_returns = pd.read_csv(fin_data_path+f\"Datastream/Data/return_{return_period}.csv\", index_col=0)                 \n",
    "asset_returns.index = pd.to_datetime(asset_returns.index)                                                        \n",
    "asset_returns = asset_returns.replace([np.inf, -np.inf, 0.00000], np.nan)              \n",
    "asset_returns = asset_returns[asset_returns.apply(lambda x: sum(x.isna()), axis=1) < 0.8*asset_returns.shape[1]] # Drop rows with more than 80% NaN values\n",
    "asset_returns = asset_returns.loc[asset_returns.index >= datetime(1994,1,1)]\n",
    "return_data = asset_returns.loc[news_event_df.index.get_level_values(0)[0]-timedelta(days=1) : news_event_df.index.get_level_values(0)[-1]]\n",
    "\n",
    "# Load Beta values (file reference: Volatility adjusted labels.ipynb)\n",
    "beta = pd.read_csv(fin_data_path+'beta_mc_to_mc_df_2022.csv', index_col=0)     \n",
    "beta.index = pd.to_datetime(beta.index)\n",
    "beta = pd.merge_asof(left=asset_returns.loc[:, []], right=beta, left_index=True, right_index=True, direction='backward')\n",
    "\n",
    "def TickerInSP500(date):\n",
    "    idx = TickerIX_W.index.get_loc(date, method='pad')\n",
    "    return list(TickerIX_W.iloc[idx].loc[(TickerIX_W.iloc[idx]==1)].index.dropna())\n",
    "\n",
    "TickerIX_W = pd.read_csv(fin_data_path+\"TickerIX_W_06_2022.csv\", index_col=0)\n",
    "TickerIX_W.index = pd.to_datetime(TickerIX_W.index)\n",
    "TickerIX_W = TickerIX_W.loc[datetime(1990,1,1):]\n",
    "\n",
    "sp500 = pd.read_csv(fin_data_path+'SPCOMP_1990_2022.csv', sep=';', index_col=0)\n",
    "sp500.index = pd.to_datetime(sp500.index) \n",
    "sp500['TOT RETURN OPEN'] = (sp500['OPENING PRICE']/sp500['PRICE INDEX'])*sp500['TOT RETURN IND']\n",
    "sp500_r = sp500.pct_change()  \n",
    "sp500_r['TR OpenClose'] = (sp500['TOT RETURN IND']  - sp500['TOT RETURN OPEN'])/sp500['TOT RETURN OPEN']\n",
    "sp500_r['TR CloseOpen'] = (sp500['TOT RETURN OPEN'] - sp500['TOT RETURN IND'].shift(1))/sp500['TOT RETURN IND'].shift(1)\n",
    "\n",
    "rf_3M = pd.read_csv(data_path+'rf_3M.csv')\n",
    "rf_3M.Date = pd.to_datetime(rf_3M.Date)\n",
    "rf_3M = rf_3M.set_index('Date')\n",
    "rf_3M = rf_3M.rename(columns={'Value': 'rf_rate'})\n",
    "rf_3M = pd.merge_asof(left=sp500.loc[:, []], right=rf_3M, left_index=True, right_index=True, direction='backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d2d8fd2-de6a-4a19-b3dc-de04d0e0793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Tickers in the News: 812\n",
      "Number of unique Tickers since 2002:  1122\n",
      "Number of unique Tickers with available return data within 2002 to 2020: 967\n",
      "Number of unique Tickers that were in the S&P500 within 2002 and 2020:   880\n",
      "Number of unique Tickers with available return data that were in the S&P500 within 2002 to 2020: 875\n",
      "Number of overnight news events: 168653\n",
      "Number of daytime news events:   109952\n"
     ]
    }
   ],
   "source": [
    "ticker_with_return_data = set((return_data.loc[:, return_data.isna().sum(axis=0) < return_data.shape[0]]).columns)\n",
    "ticker_in_sp500         = set((TickerIX_W.loc[datetime(2002,1,1):datetime(2020,2,28)].loc[:, TickerIX_W.loc[datetime(2002,1,1):datetime(2020,2,28)].sum(axis=0) > 0]).columns)\n",
    "\n",
    "print(f\"Number of unique Tickers in the News: {len(news_event_df.index.get_level_values(1).unique())}\")\n",
    "print(f\"Number of unique Tickers since 2002:  {return_data.shape[1]}\")\n",
    "print(f\"Number of unique Tickers with available return data within 2002 to 2020: {len(ticker_with_return_data)}\")\n",
    "print(f\"Number of unique Tickers that were in the S&P500 within 2002 and 2020:   {len(ticker_in_sp500)}\")\n",
    "print(f\"Number of unique Tickers with available return data that were in the S&P500 within 2002 to 2020: {len(ticker_with_return_data.intersection(ticker_in_sp500))}\")\n",
    "\n",
    "print(f\"Number of overnight news events: {news_event_df.shape[0]}\")\n",
    "print(f\"Number of daytime news events:   {109952}\")  #daytime_news.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589fabd-56a2-45cb-bb35-8528a3de92be",
   "metadata": {},
   "source": [
    "Calculate idiosyncratic returns  \n",
    "*r_idiosync = r_j - r_f - beta(j, t-1)* (r_SP500(t) - r_f)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "625dbf61-4e0f-48eb-8100-a718e86d570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if return_period   == 'mc_to_mc':\n",
    "    col = 'TOT RETURN IND'\n",
    "elif return_period == 'mo_to_mo':\n",
    "    col = 'TOT RETURN OPEN'\n",
    "elif return_period == 'mo_to_mc':\n",
    "    col = 'TR OpenClose'\n",
    "elif return_period == 'mc_to_mo':\n",
    "    col = 'TR CloseOpen'\n",
    "    \n",
    "dates = beta.index\n",
    "expected_returns = beta.multiply(sp500_r.loc[dates, col] - rf_3M.loc[dates, 'rf_rate'], axis='index').add(rf_3M.loc[dates, 'rf_rate'], axis='index')\n",
    "abnormal_returns_full = asset_returns.loc[dates].subtract(expected_returns, axis='index')\n",
    "abnormal_returns = abnormal_returns_full.loc[news_event_df.index.get_level_values(0)[0]-timedelta(days=1) : news_event_df.index.get_level_values(0)[-1]]\n",
    "#abnormal_returns.to_csv(fin_data_path+f\"Datastream/Data/return_{return_period}_idiosync.csv\", encoding='utf-8')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b5ec3-045f-4d8b-a7f7-a3cb55154388",
   "metadata": {},
   "source": [
    "Calculate z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50f21ecd-96a9-4712-9128-b5b223472d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "idiosync_z_scores = True\n",
    "idiosync = '_idiosync' if idiosync_z_scores else ''\n",
    "\n",
    "if idiosync_z_scores:\n",
    "    z_scores = get_zscores(abnormal_returns_full, idiosync_z_scores)    \n",
    "else: \n",
    "    z_scores = get_zscores(asset_returns, idiosync_z_scores)    \n",
    "    \n",
    "z_scores.to_csv(fin_data_path+f'z_values_{return_period}_127d_rw{idiosync}_2022.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce5f27-ebf4-41fa-a0ba-5d65e76698e1",
   "metadata": {},
   "source": [
    "Match z-scores with news events and asset returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e55cc5-9991-41a2-9128-f83a5f1f514b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c55463949654c6ca97e7fe0ae729580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_abnormal_returns  = True\n",
    "idiosync_z_scores     = True\n",
    "\n",
    "def match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=17.5, order_time='moo'): \n",
    "    #global Z, R, N, ZR, ZRN, dataset\n",
    "    \"\"\"\n",
    "    If no_dupl = True we have no exact timestamps of the news arrival available. In this case, the index column Date is the\n",
    "    date at market open. Thus, overnight news ar linked with the next market open date. \n",
    "    \n",
    "    For daytime news with order_time = 'moc': no_dupl = False\n",
    "    \"\"\"    \n",
    "    z_values = pd.read_csv(fin_data_path+f'z_values_mo_to_mc_127d_rw{idiosync}_2022.csv', encoding='utf-8', index_col=0)   \n",
    "    z_values.index = pd.to_datetime(z_values.index)                                   \n",
    "\n",
    "    if use_abnormal_returns:\n",
    "        return_df = abnormal_returns\n",
    "    else:\n",
    "        return_df = return_data\n",
    "    \n",
    "    news_window_h   = int(news_window)\n",
    "    news_window_min = int((news_window % 1)*60)\n",
    "    \n",
    "    dates = list(z_values.loc[datetime(2002,1,2):datetime(2020,1,31)].index)\n",
    "    news_dates = set(news_event_df.index.get_level_values(0).unique())\n",
    "    z_values_df = pd.merge(left=pd.DataFrame(index=dates), right=z_values, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    z_values  = z_values.loc[dates]\n",
    "    z_values  = z_values.stack(dropna=False)\n",
    "    return_df = pd.merge(left=pd.DataFrame(index=dates), right=return_df, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    for i, date in tqdm(enumerate(dates[:-8])):\n",
    "        if i >= 5:            \n",
    "            Z = z_values.loc[dates[i-1], slice(None)].to_frame(name='z_value').reset_index(level=0)\n",
    "            Z = Z.loc[list(set(Z.index).intersection(set(TickerInSP500(dates[i-1]))))]     # Select only assets that are in the S&P500      \n",
    "            Z.Date = date\n",
    "            R = return_df.loc[dates[i-5:i+8+1], Z.index].T\n",
    "            R.columns = r_cols\n",
    "            \n",
    "            if date in news_dates:\n",
    "                if no_dupl:\n",
    "                    if order_time == 'moo':\n",
    "                        N = news_event_df.loc[(dates[i], slice(None)), :]\n",
    "                    else:\n",
    "                        print(\"Use oder_time='moo' or set no_dupl=False\")\n",
    "                        break                    \n",
    "                else:                    \n",
    "                    if order_time == 'moo':    # Market open order\n",
    "                        rw_end   = datetime.combine(dates[i], time(9,30))\n",
    "                        rw_start = rw_end - timedelta(hours=news_window_h, minutes=news_window_min)\n",
    "                    elif order_time == 'moc':  # Market close order\n",
    "                        rw_end   = datetime.combine(dates[i], time(16,0))\n",
    "                        rw_start = rw_end - timedelta(hours=news_window_h, minutes=news_window_min)                \n",
    "                        \n",
    "                    N = news_event_df.loc[((rw_start <= news_event_df.index.get_level_values('Timestamp_ET')) & (news_event_df.index.get_level_values('Timestamp_ET') <= rw_end), slice(None)), :]            \n",
    "            else:\n",
    "                N = pd.DataFrame(columns=news_event_df.columns)\n",
    "                              \n",
    "            ZRN = Z.join([R, N.reset_index(level=0, drop=True)], how='left').reset_index()\n",
    "            \n",
    "            if i == 5:\n",
    "                dataset = ZRN  \n",
    "            else:\n",
    "                dataset = pd.concat([dataset, ZRN], axis=0)\n",
    "                      \n",
    "    dataset = dataset.rename(columns={'index':'Ticker'})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "abn           = '_abn' if use_abnormal_returns else '' \n",
    "idiosync      = '_idiosync' if idiosync_z_scores else ''\n",
    "no_duplicates = '_no_dupl'  if no_dupl else ''\n",
    "\n",
    "if return_period == 'mc_to_mc':\n",
    "    new_df = match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=6.5,  order_time='moc')    # order_time: 'moc' or 'moo'\n",
    "    new_df.to_csv(f'./Data/event_data{abn}_returns_mc_to_mc_6_5h_z_val{idiosync}{no_duplicates}{freshness}_'+str(2002)+'-'+str(2021)+'_v2.csv', encoding='utf-8-sig', index=False)  \n",
    "    \n",
    "if return_period == 'mo_to_mo':\n",
    "    new_df = match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=17.5, order_time='moo')  \n",
    "    new_df.to_csv(f'./Data/event_data{abn}_returns_mo_to_mo_17_5h_z_val{idiosync}{no_duplicates}{freshness}_'+str(2002)+'-'+str(2021)+'_v2.csv', encoding='utf-8-sig', index=False)      \n",
    "    \n",
    "if return_period == 'mo_to_mc':\n",
    "    new_df = match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=17.5, order_time='moo')  \n",
    "    new_df.to_csv(f'./Data/event_data{abn}_returns_mo_to_mc_17_5h_z_val{idiosync}{no_duplicates}{freshness}_'+str(2002)+'-'+str(2021)+'_v2.csv', encoding='utf-8-sig', index=False)   \n",
    "    \n",
    "if return_period == 'mc_to_mo':\n",
    "    new_df = match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=17.5, order_time='moo')  \n",
    "    new_df.to_csv(f'./Data/event_data{abn}_returns_mc_to_mo_17_5h_z_val{idiosync}{no_duplicates}{freshness}_'+str(2002)+'-'+str(2021)+'_v2.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770cebe-98bf-4ab4-812e-f1a28b727eac",
   "metadata": {},
   "source": [
    "Combine close-to-open and open-to-close returns in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50992cdf-a418-4b5a-9bc4-f1bc191d32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_o = beta.copy()\n",
    "beta_c = beta.copy()\n",
    "beta_o.index = beta_o.index + timedelta(hours=9, minutes=30)\n",
    "beta_c.index = beta_c.index + timedelta(hours=16, minutes=0)\n",
    "beta_oc = pd.concat([beta_o, beta_c]).sort_index()\n",
    "\n",
    "sp500_close_open = sp500_r['TR CloseOpen']\n",
    "sp500_open_close = sp500_r['TR OpenClose']\n",
    "sp500_close_open.index = sp500_close_open.index + timedelta(hours=9, minutes=30)\n",
    "sp500_open_close.index = sp500_open_close.index + timedelta(hours=16, minutes=0)\n",
    "sp500_returns = pd.concat([sp500_close_open, sp500_open_close], axis=0).sort_index()\n",
    "sp500_returns.name = 'TR_SP500'\n",
    "\n",
    "rf_3M_o = rf_3M.copy()\n",
    "rf_3M_c = rf_3M.copy()\n",
    "rf_3M_o.index = rf_3M_o.index + timedelta(hours=9, minutes=30)\n",
    "rf_3M_c.index = rf_3M_c.index + timedelta(hours=16, minutes=0)\n",
    "rf_3M_oc = pd.concat([rf_3M_o, rf_3M_c]).sort_index()\n",
    "\n",
    "R_mo_to_mc = pd.read_csv(fin_data_path+f\"Datastream/Data/return_mo_to_mc.csv\", index_col=0)          \n",
    "R_mo_to_mc.index = pd.to_datetime(R_mo_to_mc.index)                                                        \n",
    "R_mo_to_mc = R_mo_to_mc.replace([np.inf, -np.inf, 0.00000], np.nan)\n",
    "R_mo_to_mc.insert(0, 'interval', 'mo_to_mc')\n",
    "R_mo_to_mc.index = R_mo_to_mc.index + timedelta(hours=16)\n",
    "\n",
    "R_mc_to_mo = pd.read_csv(fin_data_path+f\"Datastream/Data/return_mc_to_mo.csv\", index_col=0)          \n",
    "R_mc_to_mo.index = pd.to_datetime(R_mc_to_mo.index)                                                        \n",
    "R_mc_to_mo = R_mc_to_mo.replace([np.inf, -np.inf, 0.00000], np.nan)              \n",
    "R_mc_to_mo.insert(0, 'interval', 'mc_to_mo')\n",
    "R_mc_to_mo.index = R_mc_to_mo.index + timedelta(hours=9, minutes=30)\n",
    "\n",
    "asset_returns = pd.concat([R_mo_to_mc, R_mc_to_mo], axis=0).sort_index()\n",
    "return_data = asset_returns.loc[news_event_df.index.get_level_values(0)[0]-timedelta(days=1) : news_event_df.index.get_level_values(0)[-1]]\n",
    "#R_mc_to_mo = R_mc_to_mo[R_mc_to_mo.apply(lambda x: sum(x.isna()), axis=1) < 0.8*R_mc_to_mo.shape[1]] # Drop rows with more than 80% NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e982cae5-82ca-4576-84d1-50d67909f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = beta_oc.index\n",
    "\n",
    "expected_returns = beta_oc.multiply(sp500_returns[timestamps] - rf_3M_oc.loc[timestamps, 'rf_rate'], axis='index').add(rf_3M_oc.loc[timestamps, 'rf_rate'], axis='index')\n",
    "abnormal_returns = asset_returns.loc[timestamps].subtract(expected_returns, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ed0b818-9c55-463c-bf4d-fb00f08b76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59dcf9d1d7243d08334c7e3184e8381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def match_news_with_returns(news_event_df, use_abnormal_returns, news_window=17.5, order_time='moo'):    \n",
    "    \n",
    "    return_df = abnormal_returns if use_abnormal_returns else return_data\n",
    "    \n",
    "    news_window_h   = int(news_window)\n",
    "    news_window_min = int((news_window % 1)*60)\n",
    "    timestamps = list(return_data.index)\n",
    "    dates = list(news_event_df.index.get_level_values(0).unique())\n",
    "    print(len(dates))\n",
    "    \n",
    "    z_scores_oo = pd.read_csv(fin_data_path+f'z_values_mo_to_mo_127d_rw{idiosync}_2022.csv', encoding='utf-8')      \n",
    "    z_scores_oo.Date = pd.to_datetime(z_scores_oo.Date)                                   \n",
    "    z_scores_oo = z_scores_oo.set_index('Date')  \n",
    "    z_scores_oo.index = z_scores_oo.index + timedelta(hours=9, minutes=30)\n",
    "    z_scores_oo.insert(0, 'interval', 'mo_to_mo')\n",
    "\n",
    "    z_scores_oc = pd.read_csv(fin_data_path+f'z_values_mo_to_mc_127d_rw{idiosync}_2022.csv', encoding='utf-8')      \n",
    "    z_scores_oc.Date = pd.to_datetime(z_scores_oc.Date)                                   \n",
    "    z_scores_oc = z_scores_oc.set_index('Date')  \n",
    "    z_scores_oc.index = z_scores_oc.index + timedelta(hours=16, minutes=0)\n",
    "    z_scores_oc.insert(0, 'interval', 'mo_to_mc')\n",
    "\n",
    "    z_scores = pd.concat([z_scores_oo, z_scores_oc]).sort_index()\n",
    "    z_scores = pd.merge(left=pd.DataFrame(index=timestamps), right=z_scores, left_index=True, right_index=True, how='left')\n",
    "       \n",
    "    for i, date in tqdm(enumerate(dates[:-8])):\n",
    "        if i >= 5:\n",
    "            if no_dupl:\n",
    "                if order_time == 'moo':\n",
    "                    df = news_event_df.loc[(dates[i], slice(None)), :]\n",
    "                else:\n",
    "                    print(\"Use oder_time='moo' or set no_dupl=False\")\n",
    "                    break                    \n",
    "            else:   \n",
    "                if order_time == 'moo': \n",
    "                    rw_end   = datetime.combine(date, time(9,30))\n",
    "                    rw_start = rw_end - timedelta(hours=news_window_h, minutes=news_window_min)\n",
    "\n",
    "                df = news_event_df.loc[((rw_start <= news_event_df.index.get_level_values('Timestamp_ET')) & \n",
    "                                         (news_event_df.index.get_level_values('Timestamp_ET') <= rw_end), slice(None)), \n",
    "                                         ['Date', 'Sentiment', 'freshness', 'topic_1', 'topic_2', 'topic_3', 'topic_4']]\n",
    "            \n",
    "            \n",
    "            ticker = df.index.get_level_values('Ticker').unique()\n",
    "\n",
    "            \n",
    "            for j, return_j in enumerate(r_cols):\n",
    "                for t in ['o', 'c']:\n",
    "                    col = return_j + t\n",
    "                    if t == 'o':           \n",
    "                        r_j = return_df.loc[datetime.combine(dates[i-5+j], time(9,30)), ticker].rename(col)\n",
    "                    elif t == 'c':\n",
    "                        r_j = return_df.loc[datetime.combine(dates[i-5+j], time(16,0)), ticker].rename(col)\n",
    "\n",
    "                    if ((j == 0) & (t == 'o')):\n",
    "                        values = r_j\n",
    "                    else:\n",
    "                        values = pd.concat([values, r_j], axis=1)\n",
    "\n",
    "                        \n",
    "            for j, zval_j in enumerate(z_cols):\n",
    "                for t in ['o', 'c']:\n",
    "                    col = zval_j + t\n",
    "                    if t == 'o':                     \n",
    "                        z_j = z_scores.loc[datetime.combine(dates[i-3+j], time(9,30)),  ticker].rename(col)\n",
    "                    elif t == 'c':\n",
    "                        z_j = z_scores.loc[datetime.combine(dates[i-3+j], time(16,0)),  ticker].rename(col)\n",
    "\n",
    "                    values = pd.concat([values, z_j], axis=1)\n",
    "            \n",
    "            if i == 5:\n",
    "                new_df = pd.merge(df.reset_index(), values, on='Ticker', how = 'inner')\n",
    "            else:\n",
    "                new_df = pd.concat([new_df, pd.merge(df.reset_index(), values, on='Ticker', how = 'inner')])\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "use_abnormal_returns  = True\n",
    "idiosync_z_scores     = True\n",
    "abn = '_abn' if use_abnormal_returns else '' \n",
    "idiosync = '_idiosync' if idiosync_z_scores else ''\n",
    "no_duplicates = '_no_dupl'  if no_dupl else ''\n",
    "\n",
    "new_df = match_news_with_returns(news_event_df, use_abnormal_returns, news_window=17.5, order_time='moo')\n",
    "new_df.to_csv(f'./Data/event_data{abn}_returns_oc_17_5h_z_val{idiosync}{no_duplicates}_'+str(2002)+'-'+str(2021)+'_v2.csv', encoding='utf-8-sig', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3f7b9-b27a-45d9-9429-b11c65ea3817",
   "metadata": {},
   "source": [
    "Calculate Beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b0bd22-1bfc-4fba-9d71-cb273a2a94a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b344704aa94b2f93849ba44c4c9747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate Betas\n",
    "def calcBetas(X, rw=52*2):    \n",
    "    x = X.loc[:, ~X.columns.isin(['SP500Return'])]\n",
    "    y = X.loc[:,  X.columns.isin(['SP500Return'])]\n",
    "    dates   = x.index\n",
    "    beta_df = pd.DataFrame(index=dates, columns=x.columns)\n",
    "\n",
    "    for t in tqdm(range(rw, len(dates))):\n",
    "        x_rw = x.loc[dates[t-rw]:dates[t-1], :]\n",
    "        y_rw = y.loc[dates[t-rw]:dates[t-1], :]\n",
    "        sel_ticker = (x_rw==0).sum(axis=0) < 0.9*rw     # select only those ticker with enough available observations\n",
    "        x_rw = x_rw.loc[:, sel_ticker].values\n",
    "        y_rw = y_rw.values.flatten()\n",
    "        x_rw_mean = (np.ones(rw)@x_rw)/rw\n",
    "        y_rw_mean = (np.ones(rw)@y_rw)/rw\n",
    "        cov = ((y_rw - y_rw_mean) @ (x_rw - x_rw_mean))/(rw-1)\n",
    "        var = np.var(y_rw, ddof=1)\n",
    "        beta = cov/var\n",
    "        beta_df.loc[dates[t], sel_ticker] = beta\n",
    "        \n",
    "    return beta_df\n",
    "\n",
    "# Load return data\n",
    "return_data = pd.read_csv(fin_data_path+'Datastream/Data/return_mc_to_mc.csv', index_col=0)\n",
    "return_data.index = pd.to_datetime(return_data.index)\n",
    "return_data = return_data.reindex(sorted(return_data.columns), axis=1)\n",
    "\n",
    "# S&P total return data\n",
    "sp500 = pd.read_csv(fin_data_path+'SPCOMP_1990_2022.csv', sep=';', index_col=0)\n",
    "sp500['SP500Return'] = sp500['TOT RETURN IND'].pct_change()\n",
    "sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Resample to monthly frequency\n",
    "def resample_data(df, freq='W'):\n",
    "    return (df+1).resample(freq, label='right', closed='right').prod()-1\n",
    "\n",
    "X = pd.merge(left=return_data, right=sp500.SP500Return, left_index=True, right_index=True, how='inner')\n",
    "X = resample_data(X)\n",
    "\n",
    "beta_df = calcBetas(X, rw=52*2)\n",
    "beta_df.to_csv(fin_data_path+'beta_mc_to_mc_df_2022.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f29479c-df06-404c-b8cb-960b5a28dc7b",
   "metadata": {},
   "source": [
    "Calculate close-to-open return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5deec1-efb7-416e-8df5-33777d907b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_return_open  = pd.read_csv(fin_data_path+'Datastream/Data/tot_return_open_index.csv', low_memory=False)\n",
    "tot_return_close = pd.read_csv(fin_data_path+'Datastream/Data/tot_return_close_index.csv', low_memory=False)\n",
    "tot_return_open['Date']  = pd.to_datetime(tot_return_open['Date']) \n",
    "tot_return_close['Date'] = pd.to_datetime(tot_return_close['Date']) \n",
    "tot_return_open  = tot_return_open.set_index('Date')\n",
    "tot_return_close = tot_return_close.set_index('Date')\n",
    "return_close_to_open = (tot_return_open - tot_return_close.shift(periods=1))/tot_return_close.shift(periods=1)\n",
    "return_close_to_open.to_csv(fin_data_path+'Datastream/Data/return_mc_to_mo.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188a4be-4d87-4595-9a07-4a3edfbe6dc5",
   "metadata": {},
   "source": [
    "Calculate open-to-close return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32137e66-b24e-41c1-8014-b89470f0eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_return_open  = pd.read_csv(fin_data_path+'Datastream/Data/tot_return_open_index.csv', low_memory=False)\n",
    "tot_return_close = pd.read_csv(fin_data_path+'Datastream/Data/tot_return_close_index.csv', low_memory=False)\n",
    "tot_return_open['Date']  = pd.to_datetime(tot_return_open['Date']) \n",
    "tot_return_close['Date'] = pd.to_datetime(tot_return_close['Date']) \n",
    "tot_return_open  = tot_return_open.set_index('Date')\n",
    "tot_return_close = tot_return_close.set_index('Date')\n",
    "\n",
    "return_open_to_close = (tot_return_close-tot_return_open)/tot_return_open\n",
    "return_open_to_close.to_csv(fin_data_path+'Datastream/Data/return_mo_to_mc.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e283081-4e2d-4d9d-88f2-ad15b8961fff",
   "metadata": {},
   "source": [
    "Creat Simplified News Events Dataframe  \n",
    "*Source File: 'train_valid_data_inkl_pred_fresh_w2v_topics_2002-2021.csv'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746b453-55ca-4aa0-858c-9a46006d43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_event_df = pd.read_csv(extended_model_path+'train_valid_data_inkl_pred_fresh_w2v_topics_'+str(2002)+'-'+str(2021)+'.csv', encoding='utf-8-sig', index_col=0)\n",
    "news_event_df.Timestamp_ET = pd.DatetimeIndex(news_event_df.Timestamp_ET).tz_localize(None)\n",
    "news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "news_event_df = news_event_df.sort_values((['Timestamp_ET']),ascending=True)\n",
    "news_event_df = news_event_df.set_index(['Timestamp_ET'])[:-1]\n",
    "news_event_df = news_event_df.drop(['Sentiment'], axis=1)\n",
    "news_event_df['Sentiment'] = news_event_df.positive-news_event_df.negative \n",
    "\n",
    "# Insert TradingDate column for news released during after trading hours on date t \n",
    "# to show the date t+1 of market opening on the next trading day\n",
    "news_event_df['TradingDate'] = news_event_df.Date.copy()\n",
    "\n",
    "cols = ['Date', 'TradingDate', 'Ticker', 'Sentiment', 'freshness', 'topic_1', 'topic_2', 'topic_3', 'topic_4']\n",
    "news_event_df = news_event_df[cols]\n",
    "\n",
    "trading_days   = np.array(list(return_data.index))\n",
    "news_event_df  = news_event_df.loc[news_event_df.Date.isin(trading_days[:-1])]\n",
    "\n",
    "def get_market_open_date(t):\n",
    "    return trading_days[np.where(trading_days == t)[0][0] + 1]\n",
    "\n",
    "market_open_date = news_event_df.loc[news_event_df.index.time > time(16,0), 'Date'].apply(get_market_open_date)\n",
    "news_event_df.loc[news_event_df.index.time > time(16,0), 'TradingDate'] = market_open_date.copy()\n",
    "\n",
    "news_event_df = news_event_df.reset_index()\n",
    "news_event_df = news_event_df.set_index(['Timestamp_ET', 'Ticker'])\n",
    "\n",
    "news_event_df.to_csv('./Data/news_event_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc1333-5372-43ba-9bab-4e10f98786e7",
   "metadata": {},
   "source": [
    "Remove duplicate overnight news (pre- and after midnight) from news_event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27c144cc-f1c1-4fb5-9d82-729409991f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d7fd04cde64255b0cbf8d078844357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# News published between 4pm am 12am are merged into one document and news published between\n",
    "# 12am an 9:30am are merged into a seperate document. Thus there can exist two seperate overnight \n",
    "# news documents of one company. This skript removes duplicates and calculates the mean of the sentiment \n",
    "# of two news articles.\n",
    "\n",
    "# Load News Events Data\n",
    "news_event_df = pd.read_csv('./Data/news_event_df.csv', encoding='utf-8')\n",
    "news_event_df.Timestamp_ET = pd.DatetimeIndex(news_event_df.Timestamp_ET)\n",
    "news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "news_event_df = news_event_df.set_index(['Timestamp_ET'])\n",
    "\n",
    "consider_freshness = True\n",
    "freshness = '_freshness' if consider_freshness else ''\n",
    "if consider_freshness:\n",
    "    cols = ['Ticker', 'Sentiment', 'freshness', 'topic_1', 'topic_2', 'topic_3', 'topic_4']\n",
    "else:\n",
    "    cols  = ['Ticker', 'Sentiment', 'topic_1', 'topic_2', 'topic_3', 'topic_4']\n",
    "dates = np.unique(news_event_df.index.get_level_values(0).date) \n",
    "\n",
    "\n",
    "for i in tqdm(range(1, len(dates)-1)):\n",
    "    date = dates[i]\n",
    "    c = datetime.combine(dates[i-1], time(16,0))\n",
    "    o = datetime.combine(dates[i],   time(9,30))\n",
    "    \n",
    "    not_duplicated = news_event_df.loc[c:o, cols].loc[news_event_df.loc[c:o, 'Ticker'].duplicated(keep=False)==False].reset_index(drop=True)\n",
    "    if consider_freshness:\n",
    "        duplicated = news_event_df.loc[c:o, cols].loc[news_event_df.loc[c:o, 'Ticker'].duplicated(keep=False)].groupby(['Ticker', 'freshness']).mean().reset_index()\n",
    "    else:\n",
    "        duplicated = news_event_df.loc[c:o, cols].loc[news_event_df.loc[c:o, 'Ticker'].duplicated(keep=False)].groupby('Ticker').mean().reset_index()\n",
    "    \n",
    "    not_duplicated['Date'] = dates[i]\n",
    "    duplicated['Date']     = dates[i]\n",
    "    not_duplicated['prev_close_date'] = dates[i-1]\n",
    "    duplicated['prev_close_date']     = dates[i-1]    \n",
    "    \n",
    "    if i == 1:\n",
    "        news_event_df_no_dupl = pd.concat([not_duplicated, duplicated], axis=0)\n",
    "    else:\n",
    "        news_event_df_no_dupl = pd.concat([news_event_df_no_dupl, not_duplicated, duplicated], axis=0)\n",
    "        \n",
    "        \n",
    "news_event_df_no_dupl = news_event_df_no_dupl.reset_index(drop=True)\n",
    "news_event_df_no_dupl.to_csv(f'./Data/news_event_df_no_dupl{freshness}.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d93d7c-c422-4914-8d4e-06bee06cb609",
   "metadata": {},
   "source": [
    "Merge news_event_df with close-to-open and open-to-close returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ed3dd-3dfc-4c4e-9d8e-73c08b404b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_event_df_no_dupl = load_data('./Data/news_event_df_no_dupl.csv')\n",
    "\n",
    "df = news_event_df_no_dupl.loc[:, ['prediction_value', 'prev_close_date']]\n",
    "df = df.rename(columns={'prediction_value':'sentiment'})\n",
    "df = df.sort_index()\n",
    "df.to_csv('./Data/news_event_df_no_dupl_minimal.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbcfc6a-b97a-4566-bcd0-089cfdc4ed8c",
   "metadata": {},
   "source": [
    "Match returns with z-score events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3af6f9-6eb8-4cec-9717-5991472d7d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load abnormal returns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed1298cc5264d1ab8a62924bbae2c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_values_127d_mo_to_mc_idiosync_abn_returns_mc_to_mo_2002-2021.csv\n"
     ]
    }
   ],
   "source": [
    "def z_scores_and_returns(z_values, use_abnormal_returns=False, idiosync_z_scores=False, exclude_conditional=False):    \n",
    "    if use_abnormal_returns:\n",
    "        print('Load abnormal returns')\n",
    "        return_df = abnormal_returns\n",
    "    else:\n",
    "        print('Load total returns')\n",
    "        return_df = asset_returns\n",
    "        \n",
    "    if exclude_conditional:\n",
    "        print('Exclude conditional observations ...')\n",
    "        for (ticker, date) in tqdm(test_data_pred.index.values):\n",
    "            z_values.loc[date, ticker] = np.nan\n",
    "        \n",
    "    dates    = list(z_values.loc[datetime(2002,1,2):datetime(2020,1,31)].index)\n",
    "    z_values = z_values.loc[dates]\n",
    "    z_values = z_values.stack(dropna=False)\n",
    "    return_df= pd.merge(left=pd.DataFrame(index=dates), right=return_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    \n",
    "    init = True\n",
    "    for i, date in tqdm(enumerate(dates[:-10])):\n",
    "        if i > 4:   \n",
    "            ticker_in_sp500_t = set(TickerInSP500(dates[i]))            \n",
    "            Z = z_values.loc[dates[i], slice(None)].to_frame(name='z_value').reset_index(level=0)\n",
    "            Z = Z.loc[list(set(Z.index).intersection(ticker_in_sp500_t))]          # Select only assets that are in the S&P500            \n",
    "            R = return_df.loc[dates[i-5:i+8+1], Z.index].T\n",
    "            R.columns = r_cols\n",
    "            ZR = pd.merge(left=Z, right=R, left_index=True, right_index=True, how='inner')\n",
    "            if init:\n",
    "                z_values_returns = ZR\n",
    "                init=False\n",
    "            else:\n",
    "                z_values_returns = pd.concat([z_values_returns, ZR], axis=0)\n",
    "\n",
    "    # Remove rows where all returns are missing        \n",
    "    z_values_returns = z_values_returns.loc[z_values_returns[r_cols].isna().sum(axis=1)!=len(r_cols)]        \n",
    "    \n",
    "    return z_values_returns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "use_abnormal_returns = True\n",
    "idiosync_z_scores    = True\n",
    "exclude_conditional  = False\n",
    "\n",
    "abn = '_abn' if use_abnormal_returns else '' \n",
    "idiosync = '_idiosync' if idiosync_z_scores else ''\n",
    "excl_cond = '_excl_cond' if exclude_conditional else ''\n",
    "\n",
    "z_scores = pd.read_csv(fin_data_path+f'z_values_mo_to_mc_127d_rw{idiosync}_2022.csv', encoding='utf-8')      \n",
    "z_scores.Date = pd.to_datetime(z_scores.Date)                                   \n",
    "z_scores = z_scores.set_index('Date')\n",
    "        \n",
    "\n",
    "z_values_returns = z_scores_and_returns(z_scores, \n",
    "                                        use_abnormal_returns, \n",
    "                                        idiosync_z_scores,\n",
    "                                        exclude_conditional           # exclude observations where news articles were released\n",
    "                                       )\n",
    "\n",
    "# Save the Data Frame\n",
    "filename = f'z_values_127d_mo_to_mc{idiosync}{abn}_returns_{return_period}{excl_cond}_'+str(2002)+'-'+str(2021)+'.csv'\n",
    "z_values_returns.to_csv('./Data/'+filename, encoding='utf-8-sig', index=True)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba1a68-c229-4c21-95b4-becb38012b76",
   "metadata": {},
   "source": [
    "Generate z_score_event_df file containing all z-scores together with  close-to-open and open-to-close returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "520db5b0-8d3f-4e3f-9e2c-33e3224b3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2318782, 15)\n",
      "(2316109, 15)\n"
     ]
    }
   ],
   "source": [
    "z_values_df_1 = pd.read_csv(f'./Data/z_values_127d_mo_to_mc_idiosync_abn_returns_mc_to_mo_2002-2021.csv', encoding='utf-8-sig', index_col=0)   \n",
    "z_values_df_1.index.name = 'Ticker'\n",
    "z_values_df_1.Date = pd.to_datetime(z_values_df_1.Date)\n",
    "z_values_df_1 = z_values_df_1.set_index(['Date'], append=True)\n",
    "z_values_df_1 = z_values_df_1.sort_index()\n",
    "print(z_values_df_1.shape)\n",
    "\n",
    "z_values_df_2 = pd.read_csv(f'./Data/z_values_127d_mo_to_mc_idiosync_abn_returns_mo_to_mc_2002-2021.csv', encoding='utf-8-sig', index_col=0)   \n",
    "z_values_df_2.index.name = 'Ticker'\n",
    "z_values_df_2.Date = pd.to_datetime(z_values_df_2.Date)\n",
    "z_values_df_2 = z_values_df_2.set_index(['Date'], append=True)\n",
    "z_values_df_2 = z_values_df_2.sort_index()\n",
    "print(z_values_df_2.shape)\n",
    "\n",
    "z_score_df = pd.concat([z_values_df_1[['z_value', 'return_t_1']], z_values_df_2[['return_t_1']]], axis=1)\n",
    "z_score_df.columns = ['z_score', 'return_mc_to_mo', 'return_mo_to_mc']\n",
    "z_score_df.to_csv('./Data/z_score_event_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6c793-c994-415e-b9e0-649d486dc0f7",
   "metadata": {},
   "source": [
    "Merge news events with z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142de3f4-3e46-405e-aa9e-9d270d516761",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df = pd.read_csv('./Data/z_score_event_df.csv', encoding='utf-8')\n",
    "z_score_df.Date = pd.to_datetime(z_score_df.Date)\n",
    "\n",
    "alldates = np.unique(z_score_df.Date)\n",
    "alldates = pd.DataFrame(data={'Date':alldates, 'Date_Index': np.arange(0, len(alldates))})\n",
    "\n",
    "temp1 = pd.merge(left=z_score_df, right=alldates, on='Date', how='left')\n",
    "temp1 = temp1.rename(columns={'z_score':'z_score_tp1'})\n",
    "temp1 = temp1[['Ticker', 'Date_Index', 'z_score_tp1']]\n",
    "temp1 = temp1.set_index(['Date_Index', 'Ticker'])\n",
    "\n",
    "dataset = pd.merge(left=z_score_df, right=alldates, on='Date', how='left')\n",
    "dataset['Date_Index_Merge'] = dataset.Date_Index+1\n",
    "dataset = dataset.rename(columns={'return_mc_to_mo':'return_mc_to_mo_tp1', 'return_mo_to_mc':'return_mo_to_mc_tp1'})\n",
    "dataset = pd.merge(left=dataset, right=temp1, left_on=['Date_Index_Merge', 'Ticker'], right_index=True, how='left')\n",
    "dataset = dataset.drop(['Date_Index', 'Date_Index_Merge'], axis=1)\n",
    "\n",
    "dataset.to_csv('./Data/z_score_event_df_sentiment_v2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33964d4-659f-47df-a90c-b2446b48aefe",
   "metadata": {},
   "source": [
    "Daytime News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2e53f-aaaa-41d4-8a1c-b7014647acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_events = pd.read_csv(\"./Data/news_event_df.csv\", encoding='utf-8')\n",
    "news_events.Timestamp_ET = pd.to_datetime(news_events.Timestamp_ET)\n",
    "news_events.TradingDate = pd.to_datetime(news_events.TradingDate)\n",
    "news_events.Date = pd.to_datetime(news_events.Date)\n",
    "\n",
    "daytime_news = news_events.loc[((news_events.Timestamp_ET.dt.time >= time(9,30)) & (news_events.Timestamp_ET.dt.time <= time(16,0)))==True]\n",
    "daytime_news = daytime_news.loc[:, ['Ticker', 'Date', 'Sentiment']].reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of daytime news events: {daytime_news.shape[0]}\")\n",
    "#daytime_news.to_csv('./Data/news_event_df_daytime.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c897ea0-5119-4161-bc03-45d7b904b3ba",
   "metadata": {},
   "source": [
    "S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55331f9e-d248-45cc-8128-51a276a8934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names    = ['Total Return', 'Price Index', 'Open', 'Total Return Open', 'TR OpenClose', 'TR CloseOpen']\n",
    "sp500_r_save = sp500_r.copy()\n",
    "sp500_r_save.columns = col_names\n",
    "sp500_r_save.to_csv(\"./Data/SP500_OpenCloseReturns.csv\", encoding='utf-8')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9be00f-e08d-43b3-a06c-b8eeb2acb10d",
   "metadata": {},
   "source": [
    "**Calculate news freshness**   \n",
    "Consider news as stale if very similar news articles were published in the previous open-to-close or close-to-open period. Don't consider news as stale if they are published in the same period -> intensity score = 1 if one aricle is publised in one session, 2 if two similar articles are published in the same session, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33754017-af69-4f8a-bd73-06b3b0767041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_path       = \"C:/Users/Stefa/Documents/Uni/Projektassistenz/Python/Data/\"\n",
    "path_data_files = \"F:/RTRS_News_Data/OLD_Files/filtered_news_data_for_transformer/train_validation_data/\"\n",
    "extended_model_path2 = data_path+\"Classification/DagoBERT/SCE_Loss_minw_25_lr_5e5_6ep_bs32_wd_1e2_a0_5_b3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6550e960-c370-48c3-8d32-6474a0245566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:/Users/Stefa/Documents/Uni/Projektassistenz/Python/Data/Classification/DagoBERT/SCE_Loss_minw_25_lr_5e5_6ep_bs32_wd_1e2_a0_5_b3/DagoBERT_1996-2017_news_input_seq_250/ were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at C:/Users/Stefa/Documents/Uni/Projektassistenz/Python/Data/Classification/DagoBERT/SCE_Loss_minw_25_lr_5e5_6ep_bs32_wd_1e2_a0_5_b3/DagoBERT_1996-2017_news_input_seq_250/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions\n",
    "def calc_similarity(text, MAX_LEN, batch_size):\n",
    "    encode = tokenizer.batch_encode_plus(\n",
    "        text,                                # Sentence to encode.\n",
    "        add_special_tokens = True,           # Add '[CLS]' and '[SEP]'\n",
    "        max_length = MAX_LEN,                # Pad & truncate all sentences.\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask = True,        # Construct attn. masks.\n",
    "        return_tensors = 'pt',               # Return tensorflow tensors.\n",
    "    )\n",
    "\n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_enc = (encode['input_ids'])\n",
    "    attention_masks_enc = (encode['attention_mask'])\n",
    "\n",
    "    # Convert the lists into Pytorch tensors.\n",
    "    input_ids = input_ids_enc.clone().to(device)\n",
    "    attention_masks = attention_masks_enc.clone().to(device)\n",
    "    \n",
    "    # Create the DataLoader for our training set.\n",
    "    test_dataset = TensorDataset(input_ids, attention_masks)\n",
    "    test_sampler = SequentialSampler(text)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)    \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        transformer_output = {}\n",
    "        for i, batch in enumerate(test_dataloader):\n",
    "            input_ids, attention_mask = batch[0].to(device), batch[1].to(device)\n",
    "            transformer_output[i] = model(input_ids, token_type_ids=None, attention_mask=attention_mask) \n",
    "      \n",
    "    # Pooled Output (CLS Token Embeddings)\n",
    "    for key in transformer_output.keys():\n",
    "        if key == 0:\n",
    "            sentence_embeddings = transformer_output[key][1].cpu().detach().numpy()\n",
    "        else:\n",
    "            sentence_embeddings = np.append(sentence_embeddings, transformer_output[key][1].cpu().detach().numpy(), axis=0)\n",
    "\n",
    "    return cosine_similarity(sentence_embeddings)\n",
    "\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "\n",
    "# Load DagoBERT\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(data_path+\"DagoBERT/tokenizer_v1/tokenizer_1996-2017\") \n",
    "model     = RobertaModel.from_pretrained(extended_model_path2+\"DagoBERT_1996-2017_news_input_seq_250/\")    \n",
    "model     = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f2fff5-5d63-4912-a3b3-e44e0f85f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_event_df = pd.read_csv(extended_model_path+'train_valid_data_inkl_pred_fresh_w2v_topics_'+str(2002)+'-'+str(2021)+'.csv', encoding='utf-8-sig', index_col=0)\n",
    "news_event_df.Timestamp_ET = pd.DatetimeIndex(news_event_df.Timestamp_ET).tz_localize(None)\n",
    "news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "news_event_df = news_event_df.sort_values((['Timestamp_ET']),ascending=True)\n",
    "news_event_df = news_event_df.drop(['Sentiment'], axis=1)\n",
    "news_event_df['Sentiment'] = news_event_df.positive-news_event_df.negative \n",
    "news_event_df = news_event_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5981edbd-ac88-4388-9c92-28671ffd86ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Timestamp_ET</th>\n",
       "      <th>News</th>\n",
       "      <th>freshness</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUN</td>\n",
       "      <td>2002-01-03</td>\n",
       "      <td>2002-01-03 00:09:28.021</td>\n",
       "      <td>suncorp raises mln in share plan brisbane jan ...</td>\n",
       "      <td>fresh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916798</td>\n",
       "      <td>0.024531</td>\n",
       "      <td>0.058671</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SIAL</td>\n",
       "      <td>2002-01-03</td>\n",
       "      <td>2002-01-03 02:00:25.322</td>\n",
       "      <td>sia rises on nov loads bargain hunting singapo...</td>\n",
       "      <td>fresh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.976509</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.012550</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker       Date            Timestamp_ET  \\\n",
       "0    SUN 2002-01-03 2002-01-03 00:09:28.021   \n",
       "1   SIAL 2002-01-03 2002-01-03 02:00:25.322   \n",
       "\n",
       "                                                News freshness  topic_1  \\\n",
       "0  suncorp raises mln in share plan brisbane jan ...     fresh      0.0   \n",
       "1  sia rises on nov loads bargain hunting singapo...     fresh      0.0   \n",
       "\n",
       "    topic_2  topic_3  topic_4   neutral  negative  positive  Prediction  \\\n",
       "0  0.189655      0.0      0.0  0.916798  0.024531  0.058671           0   \n",
       "1  0.172414      0.0      0.0  0.976509  0.010941  0.012550           0   \n",
       "\n",
       "   Sentiment  \n",
       "0   0.034140  \n",
       "1   0.001609  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_event_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402d405-ed32-4334-aaa9-ad18ffedfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Similarity\n",
    "cosine_threshhold  = 0.85    # Use a combination of sentence embeddings cosine distance\n",
    "jaccard_threshhold = 0.95       # and Jaccard Similariry to find fresh and stale news\n",
    "lookback = 3                    # use news published in the past 3 days to compare for similarity\n",
    "    \n",
    "news_event_df['freshness2'] = 'fresh'\n",
    "dates = news_event_df['Date'].unique()\n",
    "print('Iterations:', len(dates))\n",
    "lookback_period = {}\n",
    "time_idx = news_event_df['Timestamp_ET'].to_frame().copy()\n",
    "\n",
    "\n",
    "for i, date in tqdm(enumerate(dates)):\n",
    "    if i < 3:\n",
    "        continue\n",
    "    lookback_period['daytime']   = ((time_idx.Timestamp_ET >  pd.to_datetime(dates[i-lookback]).replace(hour=16, minute=0, second=0)) &\n",
    "                                    (time_idx.Timestamp_ET <= pd.to_datetime(dates[i]).replace(hour=16, minute=0, second=0)))     \n",
    "\n",
    "    lookback_period['overnight'] = ((time_idx.Timestamp_ET >  pd.to_datetime(dates[i-lookback]).replace(hour=9, minute=30, second=0)) &\n",
    "                                    (time_idx.Timestamp_ET <= pd.to_datetime(dates[i-1]).replace(hour=16, minute=0, second=0))) \n",
    "\n",
    "    \n",
    "    for period in lookback_period.keys():      \n",
    "        lookback_idx = time_idx.loc[lookback_period[period]].index \n",
    "\n",
    "        if period == 'daytime':\n",
    "            new_added_idx = time_idx.loc[((time_idx.Timestamp_ET >= pd.to_datetime(dates[i]).replace(hour=9, minute=30, second=0)) &\n",
    "                                          (time_idx.Timestamp_ET <= pd.to_datetime(dates[i]).replace(hour=16, minute=0, second=0)))].index\n",
    "        else:\n",
    "            new_added_idx = time_idx.loc[((time_idx.Timestamp_ET > pd.to_datetime(dates[i-1]).replace(hour=16, minute=0, second=0)) &\n",
    "                                          (time_idx.Timestamp_ET < pd.to_datetime(dates[i]).replace(hour=9, minute=30, second=0)))].index \n",
    "\n",
    "\n",
    "        batch_idx = sorted(list(set(lookback_idx).union(set(new_added_idx))))\n",
    "        cosine_distances = calc_similarity(list(news_event_df.loc[batch_idx].News.values), MAX_LEN=250, batch_size=16)        \n",
    "\n",
    "\n",
    "        for idx in new_added_idx:\n",
    "            document = news_event_df.loc[idx, 'News'].split(' ')\n",
    "            jaccard_sim = {}\n",
    "            if period == 'daytime':\n",
    "                query = np.arange(lookback_idx[0], (idx-1))\n",
    "            else:\n",
    "                query = np.array(lookback_idx)\n",
    "\n",
    "            for query_idx in query:\n",
    "                jaccard_sim[query_idx] = jaccard_similarity(news_event_df.loc[query_idx].News.split(' '), document)\n",
    "\n",
    "\n",
    "            if len(jaccard_sim) >= 1:\n",
    "                if max(jaccard_sim.values()) > jaccard_threshhold:\n",
    "\n",
    "                    similar_index   = [item[0] for item in jaccard_sim.items() if item[1] > jaccard_threshhold] \n",
    "                    similar_index_i = [i for (i, index) in enumerate(batch_idx) if index in set(similar_index)]\n",
    "                    jaccard_array   = np.array([jaccard_sim[x] for x in similar_index])\n",
    "                    cosine_array    = cosine_distances[similar_index_i]\n",
    "                    similarity      = jaccard_array*cosine_array.max(axis=1)\n",
    "\n",
    "                    if max(similarity) > jaccard_threshhold*cosine_threshhold:\n",
    "                        news_event_df.loc[idx, 'freshness2'] = 'stale'      \n",
    "\n",
    "#news_event_df.to_csv('./Data/news_event_df_freshness_v2.csv', encoding='utf-8-sig', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb9af399-d002-4cdf-9596-637ee5d973a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_event_df.loc[news_event_df.freshness2=='stale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef811f-4705-47b1-919e-f44bc66e8397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2830e09-3d78-4957-a1e6-469079458759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import KeyedVectors\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "Textual_Factors = SourceFileLoader(\"utilities.py\", \"C:/Users/Stefa/Documents/Uni/Projektassistenz/Paper III GUIDED TOPIC CLUSTERING/Textual_Factors/utilities.py\").load_module()\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "cos_similarity  = Textual_Factors.SimilarityMeasure(sim_measure='cos_similarity').calc_similarity   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0d5817-3698-42e1-988d-1a8a84ec6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec\n",
    "root_dir = 'C:/Users/Stefa/Documents/Uni/Projektassistenz/Paper III GUIDED TOPIC CLUSTERING/'\n",
    "w2v_path = root_dir+\"Textual_Factors/models/\"\n",
    "\n",
    "pca_dim    = 63\n",
    "embeddings = KeyedVectors.load(w2v_path+'w2v_cbow_64_neg_10_window_18_60_epochs_bigrams_1996_2018.word2vec')     # CBOW\n",
    "vocab      = set(embeddings.wv.key_to_index.keys())\n",
    "vocab_list   = list(vocab)\n",
    "vocab_series = pd.Series(vocab_list)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=pca_dim)\n",
    "principalComponents = pca.fit_transform(embeddings.wv[vocab_list])\n",
    "pca_embds = {}\n",
    "for i, w in enumerate(vocab_list):\n",
    "    pca_embds[w] = principalComponents[i]\n",
    "\n",
    "# Add polarity dimension\n",
    "word_polarities = pd.read_csv(root_dir+'Textual_Factors/data/word_polarities.csv', index_col=0)\n",
    "word_polarities.columns = ['polarity']\n",
    "polarity_words = set(word_polarities.index)\n",
    "for i, w in enumerate(vocab_list):\n",
    "    if w in polarity_words:\n",
    "        pca_embds[w] = Textual_Factors.Unitvec(np.append(pca_embds[w], 5*max(abs(pca_embds[w]))*word_polarities.loc[w]))\n",
    "    else:\n",
    "        pca_embds[w] = Textual_Factors.Unitvec(np.append(pca_embds[w], 0))\n",
    "        \n",
    "embeddings_dict = Textual_Factors.Get_PCA_Embds(pca_embds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "ea52ee27-745a-41f5-aeb4-8cef46a5a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blacklist():  \n",
    "    \"\"\"\n",
    "    The blacklist is a list of regular words that are included in company names.\n",
    "    Those words are excluded for calculating the article relevance\n",
    "    \"\"\"\n",
    "    blacklist = []\n",
    "    with open('C:/Users/Stefa/Documents/Uni/Projektassistenz/DEKA/Data/blacklist.txt', 'r') as f:\n",
    "        for row in f:\n",
    "            blacklist.append(re.sub(r'\\n', '', row.lower()))\n",
    "    return set(blacklist) \n",
    "\n",
    "\n",
    "def clean_cmpy_names(tickers, names):\n",
    "    drop_words = ['dead', 'delist', 'merger', 'liquidation']\n",
    "    cmpy, cmpy_indices = {}, {}\n",
    "    blacklist = load_blacklist()\n",
    "\n",
    "    for i, ticker in enumerate(tickers):\n",
    "        name = names[i]\n",
    "        cmpy[ticker] = []\n",
    "        for w in name.split()[:2]:\n",
    "            w = re.sub(r\"[^A-Za-z]\", \" \", w.lower()).strip()\n",
    "            if (len(w) >= 3) & (w not in drop_words) & (w not in blacklist) & (w in vocab):\n",
    "                cmpy[ticker].append(w)    \n",
    "    return cmpy\n",
    "\n",
    "\n",
    "fin_data = \"C:/Users/Stefa/Documents/Uni/Projektassistenz/Financial Data/\"\n",
    "constituents = pd.read_csv(fin_data+'Datastream/Data/SP500_Constituents_06_2022.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "all_cmpy   = clean_cmpy_names(constituents.BestTicker, constituents.NAME)\n",
    "cmpy_words = list(np.unique(np.concatenate(list(all_cmpy.values()))))\n",
    "cmpy_embds = embeddings_dict[cmpy_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "1f269af1-683f-4516-8aca-7c7e68718efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = 'microsoft reports surging profits'\n",
    "#query    = 'so you say the car is one that is black'\n",
    "#query = 'the firms profits surge'\n",
    "query = 'profits of microsoft and intel surge'\n",
    "\n",
    "\n",
    "#document = news_event_df.News[19]\n",
    "#query    = news_event_df.News[10]\n",
    "#query    = \"update newmont set to win normandy as anglogold holds fire newmont sweetens normandy bid again update newmont sweetens normandy bid again new york normandy mining says recommends revisednewmont mining offer by darren schuettler and sophie hares johannesburg sydney jan newmont mining looked poised to win takeover battle for australia normandy mining with sweetened bid on thursday as \"+text0\n",
    "\n",
    "doc_vecs = embeddings_dict[[w for w in document.split(' ') if w in vocab]]\n",
    "q_vecs   = embeddings_dict[[w for w in query.split(' ') if w in vocab]]\n",
    "\n",
    "doc_len, q_len = doc_vecs.shape[0], q_vecs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "0a288243-89c4-4f62-ae3c-6dec2d244dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 0.25 1\n",
      "0.37500124999949996\n"
     ]
    }
   ],
   "source": [
    "def doc_similaritiy(doc_vecs, q_vecs, simCrit=0.8):\n",
    "    if q_vecs.shape[0] >= doc_vecs.shape[0]:\n",
    "        long_seq  = q_vecs\n",
    "        short_seq = doc_vecs\n",
    "        long_len  = q_vecs.shape[0]\n",
    "        short_len = doc_vecs.shape[0]\n",
    "    else:\n",
    "        long_seq  = doc_vecs\n",
    "        short_seq = q_vecs\n",
    "        long_len  = doc_vecs.shape[0]  \n",
    "        short_len = q_vecs.shape[0]\n",
    "        \n",
    "    S = cosine_similarity(short_seq, long_seq) \n",
    "    diags = [(S.diagonal(i) > simCrit).sum() for i in range(0,S.shape[1])]\n",
    "\n",
    "    doc_in_q_ratio     = (S.max(axis=1) > simCrit).sum()/short_len    # ratio of document words contained in query document   \n",
    "    longest_seq_ratio  = max(diags)/short_len                         # the longest identical sequence of words contained in both documents relative to the document length\n",
    "    #longest_seq_ratio2 = max(diags)/max(long_len, short_len)         # the longest identical sequence of words contained in both documents relative to the longest document length\n",
    "    #length_ratio       = short_len/long_len                          # ratio of the short sequence to the long sequence\n",
    "    #return (doc_in_q_ratio, longest_seq_ratio, longest_seq_ratio2, length_ratio)\n",
    "    \n",
    "    # Find company mentions in the texts\n",
    "    a = (cosine_similarity(cmpy_embds, long_seq)  > 0.99).sum(axis=1) >= 1\n",
    "    b = (cosine_similarity(cmpy_embds, short_seq) > 0.99).sum(axis=1) >= 1\n",
    "    same_firms = int((a & b).sum() >= 1)\n",
    "        \n",
    "    return S, doc_in_q_ratio, longest_seq_ratio, same_firms\n",
    "\n",
    "\n",
    "S, doc_in_q_ratio, longest_seq_ratio, same_firms = doc_similaritiy(doc_vecs, q_vecs, simCrit=0.7)\n",
    "\n",
    "print(doc_in_q_ratio, longest_seq_ratio, same_firms)\n",
    "\n",
    "similarity = (same_firms*2)/((1/(doc_in_q_ratio+0.000001))+(1/(longest_seq_ratio+0.000001)))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f522fd-5706-42af-90d1-c6c19e0966e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d151520-af6c-41e0-b655-b7873a9ad80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02724ee0-f894-44b4-9d04-8de01cba09ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430cd81-3dfe-4a47-b58f-bc4b90085619",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_length = 30 #days\n",
    "lookback     = 3  #days, use news published in the past 3 days to compare for similarity\n",
    "\n",
    "dates = news_event_df['Date'].unique()\n",
    "news_event_df['staleness'] = 0\n",
    "\n",
    "print('Iterations:', len(dates))\n",
    "V = {}\n",
    "\n",
    "\n",
    "\n",
    "def doc_similaritiy(doc_vecs, q_vecs, simCrit=0.8):\n",
    "    if q_vecs.shape[0] >= doc_vecs.shape[0]:\n",
    "        long_seq  = q_vecs\n",
    "        short_seq = doc_vecs\n",
    "        long_len  = q_vecs.shape[0]\n",
    "        short_len = doc_vecs.shape[0]\n",
    "    else:\n",
    "        long_seq  = doc_vecs\n",
    "        short_seq = q_vecs\n",
    "        long_len  = doc_vecs.shape[0]  \n",
    "        short_len = q_vecs.shape[0]\n",
    "        \n",
    "    S = cosine_similarity(short_seq, long_seq) \n",
    "    diags = [(S.diagonal(i) > simCrit).sum() for i in range(0,S.shape[1])]\n",
    "\n",
    "    doc_in_q_ratio     = (S.max(axis=1) > simCrit).sum()/short_len         # ratio of document words contained in query document   \n",
    "    longest_seq_ratio  = max(diags)/short_len                              # the longest identical sequence of words contained in both documents relative to the document length\n",
    "    a = (cosine_similarity(cmpy_embds, long_seq)  > 0.99).sum(axis=1) >= 1 # Find company mentions in the texts\n",
    "    b = (cosine_similarity(cmpy_embds, short_seq) > 0.99).sum(axis=1) >= 1\n",
    "    same_firms = int((a & b).sum() >= 1)\n",
    "        \n",
    "    similarity = (same_firms*2)/((1/(doc_in_q_ratio+0.000001))+(1/(longest_seq_ratio+0.000001)))\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "# To do: \n",
    "# Handle transition between batches\n",
    "\n",
    "for t in range(0, len(dates[:10]), batch_length):\n",
    "    batch_dates = dates[t:(t+batch_length)]\n",
    "    X = news_event_df.loc[(news_event_df.Timestamp_ET >= batch_dates[0]) & \n",
    "                          (news_event_df.Timestamp_ET <= batch_dates[-1]), ['Timestamp_ET', 'News']]\n",
    "    lookback_period = {}\n",
    "    for i in X.index:\n",
    "        V[i] = embeddings_dict[[w for w in X.loc[i, 'News'].split(' ') if w in vocab]]\n",
    "    \n",
    "\n",
    "    for i, date in tqdm(enumerate(batch_dates)):\n",
    "        if (t==0) & (i < 3):\n",
    "            continue\n",
    "        lookback_period['daytime']   = ((X.Timestamp_ET >  pd.to_datetime(batch_dates[i-lookback]).replace(hour=16, minute=0, second=0)) &\n",
    "                                        (X.Timestamp_ET <= pd.to_datetime(batch_dates[i]).replace(hour=16, minute=0, second=0)))     \n",
    "\n",
    "        lookback_period['overnight'] = ((X.Timestamp_ET >  pd.to_datetime(batch_dates[i-lookback]).replace(hour=9, minute=30, second=0)) &\n",
    "                                        (X.Timestamp_ET <= pd.to_datetime(batch_dates[i-1]).replace(hour=16, minute=0, second=0))) \n",
    "\n",
    "\n",
    "        for period in lookback_period.keys():      \n",
    "            lookback_idx = X.loc[lookback_period[period]].index \n",
    "\n",
    "            if period == 'daytime':\n",
    "                new_added_idx = X.loc[((X.Timestamp_ET >= pd.to_datetime(batch_dates[i]).replace(hour=9, minute=30, second=0)) &\n",
    "                                       (X.Timestamp_ET <= pd.to_datetime(batch_dates[i]).replace(hour=16, minute=0, second=0)))].index\n",
    "            else:\n",
    "                new_added_idx = X.loc[((X.Timestamp_ET > pd.to_datetime(batch_dates[i-1]).replace(hour=16, minute=0, second=0)) &\n",
    "                                       (X.Timestamp_ET < pd.to_datetime(batch_dates[i]).replace(hour=9, minute=30, second=0)))].index \n",
    "\n",
    "            batch_idx = sorted(list(set(lookback_idx).union(set(new_added_idx))))\n",
    "\n",
    "            for idx in new_added_idx:\n",
    "                doc_similarity = {}\n",
    "\n",
    "                if period == 'daytime':\n",
    "                    query = np.arange(lookback_idx[0], (idx-1))\n",
    "                else:\n",
    "                    query = np.array(lookback_idx)\n",
    "\n",
    "                for query_idx in query:\n",
    "                    doc_similarity[query_idx] = doc_similaritiy(V[idx], V[query_idx])\n",
    "\n",
    "                news_event_df.loc[idx, 'staleness'] = max(doc_similarity.values())\n",
    "\n",
    "\n",
    "                                                          \n",
    "#news_event_df.to_csv('./Data/news_event_df_freshness_v2.csv', encoding='utf-8-sig', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673596f-13bd-4c64-a812-248f0a055eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
