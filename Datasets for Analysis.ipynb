{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b1dc5f-392b-4904-8ebb-339d7ad5d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta, datetime, time, date\n",
    "from matplotlib import rc\n",
    "from tqdm.notebook import tqdm\n",
    "from cycler import cycler\n",
    "tqdm().pandas()\n",
    "\n",
    "fontsize = 12\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern'], 'size': fontsize})\n",
    "rc('text', usetex=True)\n",
    "\n",
    "color_list = [\"#348ABD\",\"#A60628\",\"#7A68A6\",\"#467821\",\"#CF4457\",\"#188487\",\"#E24A33\"]\n",
    "style = {\n",
    "  \"lines.linewidth\": 2.0,\n",
    "  \"axes.edgecolor\": \"#bcbcbc\",\n",
    "  \"patch.linewidth\": 0.5,\n",
    "  \"legend.fancybox\": True,\n",
    "  \"axes.prop_cycle\": cycler('color', color_list),\n",
    "  \"axes.facecolor\": \"#ffffff\",\n",
    "  \"axes.labelsize\": \"large\",\n",
    "  \"axes.grid\": True,\n",
    "  \"patch.edgecolor\": \"#eeeeee\",\n",
    "  \"axes.titlesize\": \"x-large\",\n",
    "  \"svg.fonttype\": \"path\"}\n",
    "\n",
    "matplotlib.rcParams.update(style)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "\n",
    "data_path           = 'C:/Users/Stefa/Documents/Uni/Projektassistenz/Python/Data/'\n",
    "output_path         = 'C:/Users/Stefa/Documents/Uni/Projektassistenz/Auswertung/'\n",
    "fin_data_path       = 'C:/Users/Stefa/Documents/Uni/Projektassistenz/Financial Data/'   \n",
    "extended_model_path = data_path+'Classification/DagoBERT/SCE_Loss_minw_25_lr_5e5_3ep_bs32_wd_1e2_a0_5_b3_NN1_w2v_topics/' \n",
    "\n",
    "\n",
    "def load_data(file):\n",
    "    df = pd.read_csv(file, encoding='utf-8-sig')\n",
    "    df.Date = pd.to_datetime(df.Date)\n",
    "    df = df.set_index(['Ticker', 'Date'])\n",
    "    df = df.rename(columns={'Prediction value': 'prediction_value'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_zscores(x, idiosync_z_scores, rw=int(255/2)):\n",
    "    idiosync = '_idiosync' if idiosync_z_scores else ''\n",
    "    \n",
    "    r = x.rolling(window=rw, closed='left', min_periods=int(0.9*rw))\n",
    "    m = r.mean()\n",
    "    s = r.std()\n",
    "    z = (x-m)/s\n",
    "    z.index.name = 'Date'    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021376f-6974-4ab2-ac91-d2fce810a2ef",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f407c1de-a6ea-4ef7-b41b-827e63f07811",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dupl = True                # Load news events without duplicates\n",
    "return_period = 'mo_to_mc'    # mo_to_mo, mc_to_mc \n",
    "                              # mo_to_mc, mc_to_mo\n",
    "\n",
    "z_cols = ['z_value_t_III', 'z_value_t_II', 'z_value_t_I', 'z_value_t', 'z_value_t_1']\n",
    "r_cols = ['return_t_V', 'return_t_IV', 'return_t_III', 'return_t_II', 'return_t_I', 'return_t', 'return_t_1', \n",
    "          'return_t_2', 'return_t_3', 'return_t_4', 'return_t_5', 'return_t_6', 'return_t_7', 'return_t_8']\n",
    "n_cols = []\n",
    "\n",
    "\n",
    "# Load News Events Data\n",
    "if no_dupl:\n",
    "    news_event_df = pd.read_csv('./Data/news_event_df_no_dupl.csv', encoding='utf-8', index_col=0)\n",
    "    news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "    news_event_df = news_event_df.set_index(['Date', 'Ticker']) \n",
    "    news_event_df = news_event_df.drop(['prev_close_date'], axis=1)    \n",
    "else:\n",
    "    news_event_df = pd.read_csv('./Data/news_event_df.csv', encoding='utf-8')\n",
    "    news_event_df.Timestamp_ET = pd.DatetimeIndex(news_event_df.Timestamp_ET)\n",
    "    news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "    news_event_df = news_event_df.set_index(['Timestamp_ET', 'Ticker'])\n",
    "\n",
    "\n",
    "# Return data of all constituents (file reference: Stock Return Calculation.ipynb)     \n",
    "asset_returns = pd.read_csv(data_path + f\"return_{return_period}.csv\", index_col=0)                 \n",
    "asset_returns.index = pd.to_datetime(asset_returns.index)                                                        \n",
    "asset_returns = asset_returns.replace([np.inf, -np.inf, 0.00000], np.nan)              \n",
    "asset_returns = asset_returns[asset_returns.apply(lambda x: sum(x.isna()), axis=1) < 0.8*asset_returns.shape[1]] # Drop rows with more than 80% NaN values\n",
    "return_data = asset_returns.loc[news_event_df.index.get_level_values(0)[0]-timedelta(days=1) : news_event_df.index.get_level_values(0)[-1]]\n",
    "\n",
    "# Load Beta values (file reference: Volatility adjusted labels.ipynb)\n",
    "beta = pd.read_csv(data_path+'beta_moc_df.csv')     \n",
    "beta.Date = pd.to_datetime(beta.Date)\n",
    "beta = beta.set_index('Date')\n",
    "                                        \n",
    "Ticker_inSP = pd.read_csv(data_path+'Ticker_inSP_2020.csv', index_col=0)\n",
    "Ticker_inSP.index = pd.to_datetime(Ticker_inSP.index)\n",
    "dates_daily_freq = pd.date_range(start=datetime(2002,1,1), end=datetime(2020,2,28), freq='D').to_frame().loc[:, []]\n",
    "Ticker_inSP_adj  = pd.merge_asof(left=dates_daily_freq, right=Ticker_inSP, left_index=True, right_index=True, direction='backward')\n",
    "\n",
    "sp500 = pd.read_csv(fin_data_path+'SPCOMP_1994_2022.csv', sep=';', index_col=0)\n",
    "sp500.index = pd.to_datetime(sp500.index) \n",
    "sp500['TOT RETURN OPEN'] = (sp500['OPENING PRICE']/sp500['PRICE INDEX'])*sp500['TOT RETURN IND']\n",
    "sp500_r = sp500.pct_change()  \n",
    "sp500_r['TR OpenClose'] = (sp500['TOT RETURN IND']  - sp500['TOT RETURN OPEN'])/sp500['TOT RETURN OPEN']\n",
    "sp500_r['TR CloseOpen'] = (sp500['TOT RETURN OPEN'] - sp500['TOT RETURN IND'].shift(1))/sp500['TOT RETURN IND'].shift(1)\n",
    "\n",
    "rf_3M = pd.read_csv(data_path+'rf_3M.csv')\n",
    "rf_3M.Date = pd.to_datetime(rf_3M.Date)\n",
    "rf_3M = rf_3M.set_index('Date')\n",
    "rf_3M = rf_3M.rename(columns={'Value': 'rf_rate'})\n",
    "rf_3M = pd.merge_asof(left=sp500.loc[:, []], right=rf_3M, left_index=True, right_index=True, direction='backward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589fabd-56a2-45cb-bb35-8528a3de92be",
   "metadata": {},
   "source": [
    "Calculate idiosyncratic returns  \n",
    "*r_idiosync = r_j - r_f - beta(j, t-1)* (r_SP500(t) - r_f)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625dbf61-4e0f-48eb-8100-a718e86d570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capm_expected_returns(beta_i):\n",
    "    if return_period   == 'mc_to_mc':\n",
    "        expected_return = expected_returns.rf_rate + beta_i * (expected_returns['Total Return'] - expected_returns.rf_rate)\n",
    "    elif return_period == 'mo_to_mo':\n",
    "        expected_return = expected_returns.rf_rate + beta_i * (expected_returns['Total Return Open']  - expected_returns.rf_rate)\n",
    "    elif return_period == 'mo_to_mc':\n",
    "        expected_return = expected_returns.rf_rate + beta_i * (expected_returns['TR OpenClose'] - expected_returns.rf_rate)    \n",
    "    elif return_period == 'mc_to_mo':\n",
    "        expected_return = expected_returns.rf_rate + beta_i * (expected_returns['TR CloseOpen'] - expected_returns.rf_rate)                  \n",
    "    return expected_return\n",
    "\n",
    "if return_period   == 'mc_to_mc':\n",
    "    expected_returns = pd.merge(left=beta.shift(periods=1), right=sp500_r['Total Return'], left_index=True, right_index=True)\n",
    "elif return_period == 'mo_to_mo':\n",
    "    expected_returns = pd.merge(left=beta.shift(periods=1), right=sp500_r['Total Return Open'], left_index=True, right_index=True)\n",
    "elif return_period == 'mo_to_mc':   \n",
    "    expected_returns = pd.merge(left=beta.shift(periods=1), right=sp500_r['TR OpenClose'],  left_index=True, right_index=True)\n",
    "elif return_period == 'mc_to_mo':\n",
    "    expected_returns = pd.merge(left=beta.shift(periods=1), right=sp500_r['TR CloseOpen'],  left_index=True, right_index=True)\n",
    "\n",
    "expected_returns = pd.merge(left=expected_returns, right=rf_3M,  left_index=True, right_index=True)  \n",
    "expected_returns = expected_returns.loc[:,'MLM':'MU'].apply(capm_expected_returns)\n",
    "abnormal_returns = asset_returns.loc[expected_returns.index[0]:expected_returns.index[-1]] - expected_returns\n",
    "abnormal_returns = abnormal_returns.loc[news_event_df.index.get_level_values(0)[0]-timedelta(days=1) : news_event_df.index.get_level_values(0)[-1]]\n",
    "\n",
    "#abnormal_returns.to_csv(data_path+f\"return_{return_period}_idiosync.csv\", encoding='utf-8')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce5f27-ebf4-41fa-a0ba-5d65e76698e1",
   "metadata": {},
   "source": [
    "Match z-scores with news events and asset returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0e55cc5-9991-41a2-9128-f83a5f1f514b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a154621dcf94c2eb3c876a1c73c57aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_abnormal_returns  = True\n",
    "idiosync_z_scores     = True\n",
    "\n",
    "def match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=17.5, order_time='moo'): \n",
    "    global Z, R, N, ZR, ZRN, dataset\n",
    "    \"\"\"\n",
    "    If no_dupl = True we have no exact timestamps of the news arrival available. In this case, the index column Date is the\n",
    "    date at market open. Thus, overnight news ar linked with the next market open date. \n",
    "    \n",
    "    For daytime news with order_time = 'moc': no_dupl = False\n",
    "    \"\"\"    \n",
    "    z_values = pd.read_csv(data_path+f'z_values_mo_to_mc_127d_rw{idiosync}.csv', encoding='utf-8')   \n",
    "    z_values.Date = pd.to_datetime(z_values.Date)                                   \n",
    "    z_values = z_values.set_index('Date') \n",
    "\n",
    "    if use_abnormal_returns:\n",
    "        return_df = abnormal_returns\n",
    "    else:\n",
    "        return_df = return_data\n",
    "    \n",
    "    news_window_h   = int(news_window)\n",
    "    news_window_min = int((news_window % 1)*60)\n",
    "    \n",
    "    dates = list(z_values.loc[datetime(2002,1,2):datetime(2020,1,31)].index)\n",
    "    news_dates = set(news_event_df.index.get_level_values(0).unique())\n",
    "    z_values_df = pd.merge(left=pd.DataFrame(index=dates), right=z_values, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    z_values  = z_values.loc[dates]\n",
    "    z_values  = z_values.stack(dropna=False)\n",
    "    return_df = pd.merge(left=pd.DataFrame(index=dates), right=return_df, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    for i, date in tqdm(enumerate(dates[:-8])):\n",
    "        if i >= 5:            \n",
    "            ticker_in_sp500_t = set(Ticker_inSP_adj.loc[dates[i-1], :].dropna().values)                \n",
    "            Z = z_values.loc[dates[i-1], slice(None)].to_frame(name='z_value').reset_index(level=0)\n",
    "            Z = Z.loc[list(set(Z.index).intersection(ticker_in_sp500_t))]          # Select only assets that are in the S&P500      \n",
    "            Z.Date = date\n",
    "            R = return_df.loc[dates[i-5:i+8+1], Z.index].T\n",
    "            R.columns = r_cols\n",
    "            \n",
    "            if date in news_dates:\n",
    "                if no_dupl:\n",
    "                    if order_time == 'moo':\n",
    "                        N = news_event_df.loc[(dates[i], slice(None)), :]\n",
    "                    else:\n",
    "                        print(\"Use oder_time='moo' or set no_dupl=False\")\n",
    "                        break                    \n",
    "                else:                    \n",
    "                    if order_time == 'moo':    # Market open order\n",
    "                        rw_end   = datetime.combine(dates[i], time(9,30))\n",
    "                        rw_start = rw_end - timedelta(hours=news_window_h, minutes=news_window_min)\n",
    "                    elif order_time == 'moc':  # Market close order\n",
    "                        rw_end   = datetime.combine(dates[i], time(16,0))\n",
    "                        rw_start = rw_end - timedelta(hours=news_window_h, minutes=news_window_min)                \n",
    "                        \n",
    "                    N = news_event_df.loc[((rw_start <= news_event_df.index.get_level_values('Timestamp_ET')) & (news_event_df.index.get_level_values('Timestamp_ET') <= rw_end), slice(None)), :]            \n",
    "            else:\n",
    "                N = pd.DataFrame(columns=news_event_df.columns)\n",
    "                              \n",
    "            ZRN = Z.join([R, N.reset_index(level=0, drop=True)], how='left').reset_index()\n",
    "            \n",
    "            if i == 5:\n",
    "                dataset = ZRN  \n",
    "            else:\n",
    "                dataset = pd.concat([dataset, ZRN], axis=0)\n",
    "                      \n",
    "    dataset = dataset.rename(columns={'index':'Ticker'})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "abn           = '_abn' if use_abnormal_returns else '' \n",
    "idiosync      = '_idiosync' if idiosync_z_scores else ''\n",
    "no_duplicates = '_no_dupl'  if no_dupl else ''\n",
    "\n",
    "if return_period == 'mc_to_mc':\n",
    "    new_df = match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=6.5,  order_time='moc')    # order_time: 'moc' or 'moo'\n",
    "    new_df.to_csv(f'./Data/event_data{abn}_returns_mc_to_mc_6_5h_z_val{idiosync}{no_duplicates}_'+str(2002)+'-'+str(2021)+'.csv', encoding='utf-8-sig', index=False)  \n",
    "    \n",
    "if return_period == 'mo_to_mo':\n",
    "    new_df = match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=17.5, order_time='moo')  \n",
    "    new_df.to_csv(f'./Data/event_data{abn}_returns_mo_to_mo_17_5h_z_val{idiosync}{no_duplicates}_'+str(2002)+'-'+str(2021)+'.csv', encoding='utf-8-sig', index=False)      \n",
    "    \n",
    "if return_period == 'mo_to_mc':\n",
    "    new_df = match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=17.5, order_time='moo')  \n",
    "    new_df.to_csv(f'./Data/event_data{abn}_returns_mo_to_mc_17_5h_z_val{idiosync}{no_duplicates}_'+str(2002)+'-'+str(2021)+'_v1.csv', encoding='utf-8-sig', index=False)   \n",
    "    \n",
    "if return_period == 'mc_to_mo':\n",
    "    new_df = match_events_with_returns(news_event_df, use_abnormal_returns, no_dupl, news_window=17.5, order_time='moo')  \n",
    "    new_df.to_csv(f'./Data/event_data{abn}_returns_mc_to_mo_17_5h_z_val{idiosync}{no_duplicates}_'+str(2002)+'-'+str(2021)+'.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770cebe-98bf-4ab4-812e-f1a28b727eac",
   "metadata": {},
   "source": [
    "Combine close-to-open and open-to-close returns in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50992cdf-a418-4b5a-9bc4-f1bc191d32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_o = beta.copy()\n",
    "beta_c = beta.copy()\n",
    "beta_o.index = beta_o.index + timedelta(hours=9, minutes=30)\n",
    "beta_c.index = beta_c.index + timedelta(hours=16, minutes=0)\n",
    "beta_oc = pd.concat([beta_o, beta_c]).sort_index()\n",
    "\n",
    "sp500_close_open = sp500_r['TR CloseOpen']\n",
    "sp500_open_close = sp500_r['TR OpenClose']\n",
    "sp500_close_open.index = sp500_close_open.index + timedelta(hours=9, minutes=30)\n",
    "sp500_open_close.index = sp500_open_close.index + timedelta(hours=16, minutes=0)\n",
    "sp500_returns = pd.concat([sp500_close_open, sp500_open_close], axis=0).sort_index()\n",
    "sp500_returns.name = 'TR_SP500'\n",
    "\n",
    "rf_3M_o = rf_3M.copy()\n",
    "rf_3M_c = rf_3M.copy()\n",
    "rf_3M_o.index = rf_3M_o.index + timedelta(hours=9, minutes=30)\n",
    "rf_3M_c.index = rf_3M_c.index + timedelta(hours=16, minutes=0)\n",
    "rf_3M_oc = pd.concat([rf_3M_o, rf_3M_c]).sort_index()\n",
    "\n",
    "R_mo_to_mc = pd.read_csv(data_path + 'return_mo_to_mc.csv', index_col=0)          \n",
    "R_mo_to_mc.index = pd.to_datetime(R_mo_to_mc.index)                                                        \n",
    "R_mo_to_mc = R_mo_to_mc.replace([np.inf, -np.inf, 0.00000], np.nan)\n",
    "R_mo_to_mc.insert(0, 'interval', 'mo_to_mc')\n",
    "R_mo_to_mc.index = R_mo_to_mc.index + timedelta(hours=16)\n",
    "\n",
    "R_mc_to_mo = pd.read_csv(data_path + 'return_mc_to_mo.csv', index_col=0)          \n",
    "R_mc_to_mo.index = pd.to_datetime(R_mc_to_mo.index)                                                        \n",
    "R_mc_to_mo = R_mc_to_mo.replace([np.inf, -np.inf, 0.00000], np.nan)              \n",
    "R_mc_to_mo.insert(0, 'interval', 'mc_to_mo')\n",
    "R_mc_to_mo.index = R_mc_to_mo.index + timedelta(hours=9, minutes=30)\n",
    "\n",
    "asset_returns = pd.concat([R_mo_to_mc, R_mc_to_mo], axis=0).sort_index()\n",
    "return_data = asset_returns.loc[news_event_df.index.get_level_values(0)[0]-timedelta(days=1) : news_event_df.index.get_level_values(0)[-1]]\n",
    "#R_mc_to_mo = R_mc_to_mo[R_mc_to_mo.apply(lambda x: sum(x.isna()), axis=1) < 0.8*R_mc_to_mo.shape[1]] # Drop rows with more than 80% NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d290a73c-7b59-4b25-8fac-7924e5cd54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capm_expected_returns(beta_i):\n",
    "    expected_return = expected_returns.rf_rate + beta_i * (expected_returns['TR_SP500'] - expected_returns.rf_rate)        \n",
    "    return expected_return\n",
    "\n",
    "expected_returns = pd.merge(left=beta_oc.shift(periods=1), right=sp500_returns, left_index=True, right_index=True)\n",
    "expected_returns = pd.merge(left=expected_returns, right=rf_3M_oc, left_index=True, right_index=True)  \n",
    "\n",
    "expected_returns = expected_returns.loc[:,'MLM':'MU'].apply(capm_expected_returns)\n",
    "abnormal_returns = asset_returns.loc[expected_returns.index[0]:expected_returns.index[-1]] - expected_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed0b818-9c55-463c-bf4d-fb00f08b76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6548205caf4754abbdc4d44174fa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def match_news_with_returns(news_event_df, use_abnormal_returns, news_window=17.5, order_time='moo'):    \n",
    "    \n",
    "    return_df = abnormal_returns if use_abnormal_returns else return_data\n",
    "    \n",
    "    news_window_h   = int(news_window)\n",
    "    news_window_min = int((news_window % 1)*60)\n",
    "    timestamps = list(return_data.index)\n",
    "    dates = list(news_event_df.index.get_level_values(0).unique())\n",
    "    print(len(dates))\n",
    "    \n",
    "    z_scores_oo = pd.read_csv(data_path+f'z_values_mo_to_mo_127d_rw{idiosync}.csv', encoding='utf-8')      \n",
    "    z_scores_oo.Date = pd.to_datetime(z_scores_oo.Date)                                   \n",
    "    z_scores_oo = z_scores_oo.set_index('Date')  \n",
    "    z_scores_oo.index = z_scores_oo.index + timedelta(hours=9, minutes=30)\n",
    "    z_scores_oo.insert(0, 'interval', 'mo_to_mo')\n",
    "\n",
    "    z_scores_oc = pd.read_csv(data_path+f'z_values_mo_to_mc_127d_rw{idiosync}.csv', encoding='utf-8')      \n",
    "    z_scores_oc.Date = pd.to_datetime(z_scores_oc.Date)                                   \n",
    "    z_scores_oc = z_scores_oc.set_index('Date')  \n",
    "    z_scores_oc.index = z_scores_oc.index + timedelta(hours=16, minutes=0)\n",
    "    z_scores_oc.insert(0, 'interval', 'mo_to_mc')\n",
    "\n",
    "    z_scores = pd.concat([z_scores_oo, z_scores_oc]).sort_index()\n",
    "    z_scores = pd.merge(left=pd.DataFrame(index=timestamps), right=z_scores, left_index=True, right_index=True, how='left')\n",
    "\n",
    "        \n",
    "    for i, date in tqdm(enumerate(dates[:-8])):\n",
    "        if i >= 5:\n",
    "            if no_dupl:\n",
    "                if order_time == 'moo':\n",
    "                    df = news_event_df.loc[(dates[i], slice(None)), :]\n",
    "                else:\n",
    "                    print(\"Use oder_time='moo' or set no_dupl=False\")\n",
    "                    break                    \n",
    "            else:   \n",
    "                if order_time == 'moo': \n",
    "                    rw_end   = datetime.combine(date, time(9,30))\n",
    "                    rw_start = rw_end - timedelta(hours=news_window_h, minutes=news_window_min)\n",
    "\n",
    "                df = news_event_df.loc[((rw_start <= news_event_df.index.get_level_values('Timestamp_ET')) & \n",
    "                                         (news_event_df.index.get_level_values('Timestamp_ET') <= rw_end), slice(None)), \n",
    "                                         ['Date', 'Sentiment', 'freshness', 'topic_1', 'topic_2', 'topic_3', 'topic_4']]\n",
    "            \n",
    "            \n",
    "            ticker = df.index.get_level_values('Ticker').unique()\n",
    "\n",
    "            for j, return_j in enumerate(r_cols):\n",
    "                for t in ['o', 'c']:\n",
    "                    col = return_j + t\n",
    "                    if t == 'o':           \n",
    "                        r_j = return_df.loc[datetime.combine(dates[i-5+j], time(9,30)), ticker].rename(col)\n",
    "                    elif t == 'c':\n",
    "                        r_j = return_df.loc[datetime.combine(dates[i-5+j], time(16,0)), ticker].rename(col)\n",
    "\n",
    "                    if ((j == 0) & (t == 'o')):\n",
    "                        values = r_j\n",
    "                    else:\n",
    "                        values = pd.concat([values, r_j], axis=1)\n",
    "\n",
    "                        \n",
    "            for j, zval_j in enumerate(z_cols):\n",
    "                for t in ['o', 'c']:\n",
    "                    col = zval_j + t\n",
    "                    if t == 'o':                     \n",
    "                        z_j = z_scores.loc[datetime.combine(dates[i-3+j], time(9,30)),  ticker].rename(col)\n",
    "                    elif t == 'c':\n",
    "                        z_j = z_scores.loc[datetime.combine(dates[i-3+j], time(16,0)),  ticker].rename(col)\n",
    "\n",
    "                    values = pd.concat([values, z_j], axis=1)\n",
    "            \n",
    "            if i == 5:\n",
    "                new_df = pd.merge(df.reset_index(), values, on='Ticker', how = 'inner')\n",
    "            else:\n",
    "                new_df = pd.concat([new_df, pd.merge(df.reset_index(), values, on='Ticker', how = 'inner')])\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "use_abnormal_returns  = True\n",
    "idiosync_z_scores     = True\n",
    "abn = '_abn' if use_abnormal_returns else '' \n",
    "idiosync = '_idiosync' if idiosync_z_scores else ''\n",
    "no_duplicates = '_no_dupl'  if no_dupl else ''\n",
    "\n",
    "new_df = match_news_with_returns(news_event_df, use_abnormal_returns, news_window=17.5, order_time='moo')\n",
    "new_df.to_csv(f'./Data/event_data{abn}_returns_oc_17_5h_z_val{idiosync}{no_duplicates}_'+str(2002)+'-'+str(2021)+'.csv', encoding='utf-8-sig', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e638cef-52f7-40d5-8792-916787322a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0843a13-add9-4a12-a836-76747169d241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439a0b0-6f4b-44b5-8880-b437016351cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5b3f7b9-b27a-45d9-9429-b11c65ea3817",
   "metadata": {},
   "source": [
    "Calculate Beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d96503-db60-4556-b4bf-6be1d6aff614",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_w  = 252*2  # rolling window of 2 years\n",
    "beta = pd.DataFrame(data=np.nan, index=return_data.index, columns=return_data.columns)\n",
    "\n",
    "\n",
    "# Load return data\n",
    "return_data = pd.read_csv(data_path + 'return_mo_to_mo.csv')\n",
    "return_data.Date = pd.to_datetime(return_data.Date)\n",
    "return_data = return_data.set_index('Date')\n",
    "\n",
    "# S&P total return data\n",
    "sp500 = pd.read_csv(data_path+'SP500_historical.csv')\n",
    "sp500['sp500_return'] = sp500['Adj Close'].pct_change()\n",
    "sp500.Date = pd.to_datetime(sp500.Date)\n",
    "sp500 = sp500.set_index('Date')\n",
    "sp500 = sp500[sp500.index >= pd.Timestamp(1990,1,2)]\n",
    "sp500 = sp500[sp500.index <= pd.Timestamp(2020,11,20)]\n",
    "\n",
    "return_data = pd.concat([return_data, sp500.sp500_return], axis=1, join='outer', sort=False)\n",
    "return_data = return_data.replace([np.inf, -np.inf, 0.00000], np.nan)\n",
    "\n",
    "\n",
    "for row in tqdm(range(r_w+1, len(return_data.index))):     \n",
    "    dateStr = return_data.index[row]\n",
    "    \n",
    "    # Rolling window\n",
    "    return_data_rw = return_data.iloc[(row-1)-r_w:(row-1), :]\n",
    "    \n",
    "    # Select all tickers that have at max. 40% NaN values in the rolling window  \n",
    "    sel_ticker = return_data_rw.isna().sum() < 0.4*r_w  \n",
    "    return_data_sel = return_data_rw[return_data.columns[sel_ticker]]\n",
    "\n",
    "    # 3. Calculate covariances cov(r_i, r_m)\n",
    "    cov = return_data_sel.cov() \n",
    "\n",
    "    # 4. Calculate var(r_m)\n",
    "    var_r_m = np.var(return_data_rw.sp500_return.dropna())\n",
    "\n",
    "    beta.loc[dateStr, :] = cov.sp500_return/var_r_m\n",
    "        \n",
    "beta = beta[r_w+1:]        \n",
    "beta.to_csv(data_path+'beta_moc_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b5ec3-045f-4d8b-a7f7-a3cb55154388",
   "metadata": {},
   "source": [
    "Calculate z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f21ecd-96a9-4712-9128-b5b223472d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "idiosync_z_scores = True\n",
    "idiosync = '_idiosync' if idiosync_z_scores else ''\n",
    "\n",
    "if idiosync_z_scores:\n",
    "    z_scores = get_zscores(abnormal_returns, idiosync_z_scores)    \n",
    "else: \n",
    "    z_scores = get_zscores(asset_returns, idiosync_z_scores)    \n",
    "    \n",
    "z_scores.to_csv(data_path+f'z_values_mo_to_mo_127d_rw{idiosync}.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f29479c-df06-404c-b8cb-960b5a28dc7b",
   "metadata": {},
   "source": [
    "Calculate close-to-open return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da5deec1-efb7-416e-8df5-33777d907b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_return_open  = pd.read_csv(data_path+'tot_return_open_index.csv', low_memory=False)\n",
    "tot_return_close = pd.read_csv(data_path+'tot_return_close_index.csv', low_memory=False)\n",
    "tot_return_open['Date']  = pd.to_datetime(tot_return_open['Date']) \n",
    "tot_return_close['Date'] = pd.to_datetime(tot_return_close['Date']) \n",
    "tot_return_open  = tot_return_open.set_index('Date')\n",
    "tot_return_close = tot_return_close.set_index('Date')\n",
    "return_close_to_open = (tot_return_open - tot_return_close.shift(periods=1))/tot_return_close.shift(periods=1)\n",
    "return_close_to_open.to_csv(data_path+'return_mc_to_mo.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188a4be-4d87-4595-9a07-4a3edfbe6dc5",
   "metadata": {},
   "source": [
    "Calculate open-to-close return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32137e66-b24e-41c1-8014-b89470f0eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_return_open  = pd.read_csv(data_path+'tot_return_open_index.csv', low_memory=False)\n",
    "tot_return_close = pd.read_csv(data_path+'tot_return_close_index.csv', low_memory=False)\n",
    "tot_return_open['Date']  = pd.to_datetime(tot_return_open['Date']) \n",
    "tot_return_close['Date'] = pd.to_datetime(tot_return_close['Date']) \n",
    "tot_return_open  = tot_return_open.set_index('Date')\n",
    "tot_return_close = tot_return_close.set_index('Date')\n",
    "\n",
    "return_open_to_close = (tot_return_close-tot_return_open)/tot_return_open\n",
    "return_open_to_close.to_csv(data_path+'return_mo_to_mc.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e283081-4e2d-4d9d-88f2-ad15b8961fff",
   "metadata": {},
   "source": [
    "Creat Simplified News Events Dataframe  \n",
    "*Source File: 'train_valid_data_inkl_pred_fresh_w2v_topics_2002-2021.csv'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c746b453-55ca-4aa0-858c-9a46006d43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_event_df = pd.read_csv(extended_model_path+'train_valid_data_inkl_pred_fresh_w2v_topics_'+str(2002)+'-'+str(2021)+'.csv', encoding='utf-8-sig', index_col=0)\n",
    "news_event_df.Timestamp_ET = pd.DatetimeIndex(news_event_df.Timestamp_ET).tz_localize(None)\n",
    "news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "news_event_df = news_event_df.sort_values((['Timestamp_ET']),ascending=True)\n",
    "news_event_df = news_event_df.set_index(['Timestamp_ET'])[:-1]\n",
    "news_event_df = news_event_df.drop(['Sentiment'], axis=1)\n",
    "news_event_df['Sentiment'] = news_event_df.positive-news_event_df.negative \n",
    "\n",
    "# Insert TradingDate column for news released during after trading hours on date t \n",
    "# to show the date t+1 of market opening on the next trading day\n",
    "news_event_df['TradingDate'] = news_event_df.Date.copy()\n",
    "\n",
    "cols = ['Date', 'TradingDate', 'Ticker', 'Sentiment', 'freshness', 'topic_1', 'topic_2', 'topic_3', 'topic_4']\n",
    "news_event_df = news_event_df[cols]\n",
    "\n",
    "trading_days   = np.array(list(return_data.index))\n",
    "news_event_df  = news_event_df.loc[news_event_df.Date.isin(trading_days[:-1])]\n",
    "\n",
    "def get_market_open_date(t):\n",
    "    return trading_days[np.where(trading_days == t)[0][0] + 1]\n",
    "\n",
    "market_open_date = news_event_df.loc[news_event_df.index.time > time(16,0), 'Date'].apply(get_market_open_date)\n",
    "news_event_df.loc[news_event_df.index.time > time(16,0), 'TradingDate'] = market_open_date.copy()\n",
    "\n",
    "news_event_df = news_event_df.reset_index()\n",
    "news_event_df = news_event_df.set_index(['Timestamp_ET', 'Ticker'])\n",
    "\n",
    "news_event_df.to_csv('./Data/news_event_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc1333-5372-43ba-9bab-4e10f98786e7",
   "metadata": {},
   "source": [
    "Remove duplicate overnight news (pre- and after midnight) from news_event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "27c144cc-f1c1-4fb5-9d82-729409991f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a54b7b44934fe988087e457f2bb5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# News published between 4pm am 12am are merged into one document and news published between\n",
    "# 12am an 9:30am are merged into a seperate document. Thus there can exist two seperate overnight \n",
    "# news documents of one company. This skript removes duplicates and calculates the mean of the sentiment \n",
    "# of two news articles.\n",
    "\n",
    "# Load News Events Data\n",
    "news_event_df = pd.read_csv('./Data/news_event_df.csv', encoding='utf-8')\n",
    "news_event_df.Timestamp_ET = pd.DatetimeIndex(news_event_df.Timestamp_ET)\n",
    "news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "news_event_df = news_event_df.set_index(['Timestamp_ET'])\n",
    "\n",
    "cols = ['Ticker', 'Sentiment', 'topic_1', 'topic_2', 'topic_3', 'topic_4']\n",
    "dates = np.unique(news_event_df.index.get_level_values(0).date) \n",
    "\n",
    "for i in tqdm(range(1, len(dates)-1)):\n",
    "    date = dates[i]\n",
    "    c = datetime.combine(dates[i-1], time(16,0))\n",
    "    o = datetime.combine(dates[i],   time(9,30))\n",
    "    \n",
    "    not_duplicated = news_event_df.loc[c:o, cols].loc[news_event_df.loc[c:o, 'Ticker'].duplicated(keep=False)==False].reset_index(drop=True)\n",
    "    duplicated     = news_event_df.loc[c:o, cols].loc[news_event_df.loc[c:o, 'Ticker'].duplicated(keep=False)].groupby('Ticker').mean().reset_index()\n",
    "    \n",
    "    not_duplicated['Date'] = dates[i]\n",
    "    duplicated['Date']     = dates[i]\n",
    "    not_duplicated['prev_close_date'] = dates[i-1]\n",
    "    duplicated['prev_close_date']     = dates[i-1]    \n",
    "    \n",
    "    if i == 1:\n",
    "        news_event_df_no_dupl = pd.concat([not_duplicated, duplicated], axis=0)\n",
    "    else:\n",
    "        news_event_df_no_dupl = pd.concat([news_event_df_no_dupl, not_duplicated, duplicated], axis=0)\n",
    "        \n",
    "        \n",
    "news_event_df_no_dupl = news_event_df_no_dupl.reset_index(drop=True)\n",
    "news_event_df_no_dupl.to_csv('./Data/news_event_df_no_dupl.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d93d7c-c422-4914-8d4e-06bee06cb609",
   "metadata": {},
   "source": [
    "Merge news_event_df with close-to-open and open-to-close returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e80ed3dd-3dfc-4c4e-9d8e-73c08b404b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_event_df_no_dupl = load_data('./Data/news_event_df_no_dupl.csv')\n",
    "\n",
    "df = news_event_df_no_dupl.loc[:, ['prediction_value', 'prev_close_date']]\n",
    "df = df.rename(columns={'prediction_value':'sentiment'})\n",
    "df = df.sort_index()\n",
    "df.to_csv('./Data/news_event_df_no_dupl_minimal.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba1a68-c229-4c21-95b4-becb38012b76",
   "metadata": {},
   "source": [
    "Generate z_score_event_df file containing all z-scores together with  close-to-open and open-to-close returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "520db5b0-8d3f-4e3f-9e2c-33e3224b3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2308915, 15)\n"
     ]
    }
   ],
   "source": [
    "zvals_returns1 = pd.read_csv(extended_model_path+'z_values_127d_mo_to_mc_idiosync_abn_returns_mc_to_mo_2002-2021.csv', encoding='utf-8-sig', index_col=0)   # z_values_returns_z_val_neg_95_\n",
    "zvals_returns1.index.name = 'Ticker'\n",
    "zvals_returns1.Date = pd.to_datetime(zvals_returns1.Date)\n",
    "zvals_returns1 = zvals_returns1.set_index(['Date'], append=True)\n",
    "zvals_returns1 = zvals_returns1.sort_index()\n",
    "print(zvals_returns1.shape)\n",
    "\n",
    "zvals_returns2 = pd.read_csv(extended_model_path+'z_values_127d_mo_to_mc_idiosync_abn_returns_mo_to_mc_2002-2021.csv', encoding='utf-8-sig', index_col=0)   # z_values_returns_z_val_neg_95_\n",
    "zvals_returns2.index.name = 'Ticker'\n",
    "zvals_returns2.Date = pd.to_datetime(zvals_returns2.Date)\n",
    "zvals_returns2 = zvals_returns2.set_index(['Date'], append=True)\n",
    "zvals_returns2 = zvals_returns2.sort_index()\n",
    "print(zvals_returns2.shape)\n",
    "\n",
    "z_score_df = pd.concat([zvals_returns1[['z_value', 'return_t_1']], zvals_returns2['return_t_1']], axis=1)\n",
    "z_score_df.columns = ['z_score', 'return_mc_to_mo', 'return_mo_to_mc']\n",
    "z_score_df.to_csv('./Data/z_score_event_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6c793-c994-415e-b9e0-649d486dc0f7",
   "metadata": {},
   "source": [
    "Merge news events with z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7c055244-5e0d-499f-a8b4-d6e024431707",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_df = pd.read_csv('./Data/z_score_event_df.csv', encoding='utf-8')\n",
    "z_score_df.Date = pd.to_datetime(z_score_df.Date)\n",
    "\n",
    "news_event_df = pd.read_csv('./Data/news_event_df_no_dupl_minimal.csv', encoding='utf-8')\n",
    "news_event_df = news_event_df.drop(['prev_close_date'], axis=1)\n",
    "news_event_df.Date = pd.to_datetime(news_event_df.Date)\n",
    "\n",
    "alldates = np.unique(z_score_df.Date)\n",
    "alldates = pd.DataFrame(data={'Date':alldates, 'Date_Index': np.arange(0, len(alldates))})\n",
    "\n",
    "temp1 = pd.merge(left=z_score_df, right=alldates, on='Date', how='left')\n",
    "temp1 = temp1.rename(columns={'z_score':'z_score_tp1'})\n",
    "temp1 = temp1[['Ticker', 'Date_Index', 'z_score_tp1']]\n",
    "temp1 = temp1.set_index(['Date_Index', 'Ticker'])\n",
    "\n",
    "temp2 = pd.merge(left=news_event_df, right=alldates, on='Date', how='left')\n",
    "temp2 = temp2.drop('Date', axis=1)\n",
    "temp2 = temp2.set_index(['Date_Index', 'Ticker'])\n",
    "\n",
    "dataset = pd.merge(left=z_score_df, right=alldates, on='Date', how='left')\n",
    "dataset['Date_Index_Merge'] = dataset.Date_Index+1\n",
    "dataset = dataset.rename(columns={'return_mc_to_mo':'return_mc_to_mo_tp1', 'return_mo_to_mc':'return_mo_to_mc_tp1'})\n",
    "dataset = pd.merge(left=dataset, right=temp1, left_on=['Date_Index_Merge', 'Ticker'], right_index=True, how='left')\n",
    "dataset = pd.merge(left=dataset, right=temp2, left_on=['Date_Index_Merge', 'Ticker'], right_index=True, how='left')\n",
    "dataset = dataset.drop(['Date_Index', 'Date_Index_Merge'], axis=1)\n",
    "\n",
    "dataset.to_csv('./Data/z_score_event_df_sentiment.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33964d4-659f-47df-a90c-b2446b48aefe",
   "metadata": {},
   "source": [
    "Daytime News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2db2e53f-aaaa-41d4-8a1c-b7014647acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_events = pd.read_csv(\"./Data/news_event_df.csv\", encoding='utf-8')\n",
    "news_events.Timestamp_ET = pd.to_datetime(news_events.Timestamp_ET)\n",
    "news_events.TradingDate = pd.to_datetime(news_events.TradingDate)\n",
    "news_events.Date = pd.to_datetime(news_events.Date)\n",
    "\n",
    "daytime_news = news_events.loc[((news_events.Timestamp_ET.dt.time >= time(9,30)) & (news_events.Timestamp_ET.dt.time <= time(16,0)))==True]\n",
    "daytime_news = daytime_news.loc[:, ['Ticker', 'Date', 'Sentiment']].reset_index(drop=True)\n",
    "\n",
    "daytime_news.to_csv('./Data/news_event_df_daytime.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
